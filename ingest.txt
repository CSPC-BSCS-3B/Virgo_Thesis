ingest.py

def chunk_documents(docs: List[Document], chunk_size: int = 2000, chunk_overlap: int = 200) -> List[Document]:
    """
    Splits documents into semantic chunks using thesis-specific separators.
    Prioritizes keeping academic sections (Abstract, Methodology, etc.) intact.
    """
    separators = [
        # Primary: Thesis structure markers
        "\nChapter ", "\nCHAPTER ",
        "\nAbstract", "\nABSTRACT",
        "\nMethodology", "\nMETHODOLOGY",
        "\nResults and Discussion", "\nRESULTS AND DISCUSSION",
        "\nConclusion", "\nCONCLUSION",
        
        # Secondary: Standard text boundaries
        "\n## ", "\n\n", "\n", ". ", " "
    ]
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators,
        length_function=token_len,
        add_start_index=True,
    )
    return splitter.split_documents(docs)

def enhance_thesis_metadata(chunks: List[Document]) -> List[Document]:
    """
    Enriches document chunks with semantic metadata by classifying content 
    into specific thesis sections (e.g., Introduction, Literature Review).
    """
    enhanced_chunks = []
    
    for chunk in chunks:
        content = chunk.page_content
        metadata = chunk.metadata.copy()
        content_lower = content.lower()
        
        # Heuristic classification of thesis sections
        if any(k in content_lower for k in ["abstract", "summary"]):
            metadata["content_type"] = "abstract"
            metadata["priority"] = "high"
            
        elif any(k in content_lower for k in ["chapter 1", "introduction"]):
            metadata["content_type"] = "introduction"
            metadata["chapter"] = "1"
            
        elif any(k in content_lower for k in ["chapter 3", "methodology"]):
            metadata["content_type"] = "methodology"
            metadata["chapter"] = "3"
            
        elif any(k in content_lower for k in ["chapter 4", "results"]):
            metadata["content_type"] = "results"
            metadata["chapter"] = "4"
            
        else:
            metadata["content_type"] = "general"
        
        enhanced_chunks.append(Document(page_content=content, metadata=metadata))
    
    return enhanced_chunks

def build_faiss_index(chunks: List[Document], persist_dir: str = "index") -> None:
    """
    Generates vector embeddings using the 'all-MiniLM-L6-v2' model and 
    builds a FAISS index for efficient similarity search.
    """
    model_name = "all-MiniLM-L6-v2"
    
    # Initialize HuggingFace embeddings (optimized for local/GPU execution)
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # Create and persistdef chunk_documents(docs: List[Document], chunk_size: int = 2000, chunk_overlap: int = 200) -> List[Document]:
    """
    Splits documents into semantic chunks using thesis-specific separators.
    Prioritizes keeping academic sections (Abstract, Methodology, etc.) intact.
    """
    separators = [
        # Primary: Thesis structure markers
        "\nChapter ", "\nCHAPTER ",
        "\nAbstract", "\nABSTRACT",
        "\nMethodology", "\nMETHODOLOGY",
        "\nResults and Discussion", "\nRESULTS AND DISCUSSION",
        "\nConclusion", "\nCONCLUSION",
        
        # Secondary: Standard text boundaries
        "\n## ", "\n\n", "\n", ". ", " "
    ]
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators,
        length_function=token_len,
        add_start_index=True,
    )
    return splitter.split_documents(docs)

def enhance_thesis_metadata(chunks: List[Document]) -> List[Document]:
    """
    Enriches document chunks with semantic metadata by classifying content 
    into specific thesis sections (e.g., Introduction, Literature Review).
    """
    enhanced_chunks = []
    
    for chunk in chunks:
        content = chunk.page_content
        metadata = chunk.metadata.copy()
        content_lower = content.lower()
        
        # Heuristic classification of thesis sections
        if any(k in content_lower for k in ["abstract", "summary"]):
            metadata["content_type"] = "abstract"
            metadata["priority"] = "high"
            
        elif any(k in content_lower for k in ["chapter 1", "introduction"]):
            metadata["content_type"] = "introduction"
            metadata["chapter"] = "1"
            
        elif any(k in content_lower for k in ["chapter 3", "methodology"]):
            metadata["content_type"] = "methodology"
            metadata["chapter"] = "3"
            
        elif any(k in content_lower for k in ["chapter 4", "results"]):
            metadata["content_type"] = "results"
            metadata["chapter"] = "4"
            
        else:
            metadata["content_type"] = "general"
        
        enhanced_chunks.append(Document(page_content=content, metadata=metadata))
    
    return enhanced_chunks

def build_faiss_index(chunks: List[Document], persist_dir: str = "index") -> None:
    """
    Generates vector embeddings using the 'all-MiniLM-L6-v2' model and 
    builds a FAISS index for efficient similarity search.
    """
    model_name = "all-MiniLM-L6-v2"
    
    # Initialize HuggingFace embeddings (optimized for local/GPU execution)
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # Create and persist