\chapter{Methodology}
\begin{refsection}
 
This chapter discusses the specific steps and logical procedures that were employed to develop and evaluate the RAG-based LLM chatbot system. This includes the research design, theoretical and mathematical framework, software and hardware tools, instruments, procedures, evaluation metrics, and a conceptual framework.

\section{Research Design}

Constructive research approach focuses on producing innovative constructions that intend to solve real-world problems and contribute to the theory of the discipline in which it is applied. This methodology is particularly well-suited to fields like information systems and artificial intelligence, where the goal is not only theoretical insight but also the creation of innovative, functional systems \cite{lukka2003cons}.


This study adopted a constructive research design to develop a library chatbot using RAG for CSPC. This approach suited the study well as it involves addressing the challenges being faced by researchers in searching and retrieving universities' theses by replacing the current yet traditional database and keyword-based search with a vector database and RAG framework, enabling a conversational and topic-oriented approach. Furthermore, the system was deployed to the cloud, allowing students to access thesis everywhere they are, since current library policies restrict users from taking physical thesis books outside the premises.

By adapting a constructive design, the researchers were able to build on existing methodologies while innovating and establishing a new solution that addressed the specific challenges in the current search and thesis retrieval in CSPC. By combining the existing information within the tailored architecture, this study contributed to the demanding research on LLMs and RAG being used in various domains, such as education and research discovery.

\section{Theorems, Algorithms, and Mathematical Models}

This study used RAG pipeline, integrated with Gemini 2.5 flash LLM for reasoning that is stored to a vector database. These enabled efficient information retrieval and generation in the context of literature and thesis search within the CSPC Library.

\subsection{Retrieval-Augmented Generation (RAG) Pipeline}

The RAG pipeline is a hybrid architecture that combines information retrieval with natural language generation. It allows LLMs to access external documents during inference, thereby improving both accuracy and contextual relevance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/rag.png}
    \caption{Basic RAG Pipeline by Dr. Julija}
    \label{fig:rag}
\end{figure}

The RAG pipeline as illustrated in \ref{fig:rag}, consists of the following key stages:

\subsubsection{A. Data Indexing}

The process begins with data loading, where all needed thesis documents are imported to be utilized.  Considering that every thesis documents are large document composing of hundreds of pages, these were splitted into smaller chunks using a token-based method that respects academic structure (Abstract; Chapters 1–5). Each chunk is then converted into a vector using the open-source ‘sentence-transformers/all-MiniLM-L6-v2‘ embedding model from Hugging Face, which was chosen for its lightweight architecture and strong semantic representation capabilities. Lastly, the data storing, where vector embeddings and their associated metadata were stored in FAISS for efficient similarity search.


\subsubsection{B. Retrieval and Generation}

This stage is where the chatbot creates an action that addresses the challenge. It begins when a user starts to query, and that query was embedded using the same model (sentence-transformers/all-MiniLM-L6-v2) used from the first stage. That embedded query was used by FAISS retriever to perform a semantic search matching from the vector database.  The parameter on Top-K most relevant chunks was set as default K =50, to balance precision and recall. And finally, during generation, these retrieved chunks are fed to the Google Gemini 2.5-flash language model as grounded context to create a human-like response.

\subsection{Large Language Model}

Based on the study of \citeauthor{naveed2024} [\citeyear{naveed2024}], LLMs have demonstrated remarkable performance in challenges related to NLP and beyond. These were also considered as cutting-edge AI systems trained on massive datasets to process and generate text and can excel in tasks such as summarization, question answering, and retrieval \cite{10.1162/daed_a_01909}.

\subsubsection{Gemini 2.5-flash}

Specifically, the llm that was used in this project is Gemini 2.5 Flash, part of the Gemini 2.X model family introduced by \citeauthor{comanici2025gem} [\citeyear{comanici2025gem}]. It gives advanced reasoning capability at a lower compute resource, low latency and cost. This family also top-ranks in the llms that have multimodal support and extended context windows \cite{comanici2025gem}. By incorporating it into this project, it can enhance thesis retrieval tailored for CSPC library users.

\section{Materials and Statistical Tools / Evaluation Methods}

To ensure optimal performance of the RAG-based LLM system, several key hardware and software components are required.

\subsection{Research Materials}

This section includes the dataset that was used, as well as the minimum hardware and software needed for the development of the system.

\subsubsection{Dataset}

This study utilized a dataset containing all available undergraduate thesis (initially 290+ pdfs) from various CSPC departments, excluding Computer Science and College of Engineering and Architecture due to unavailability. Good to note here that the system was also designed to ingest new theses, by allowing admin to upload new PDF data.

\subsubsection{Hardware}

To support the development of the RAG chatbot, the researchers used hardware components that meet or exceed the specifications listed in \ref{tab:hardware_requirements}.  These components were selected to ensure a smooth ingestion and embedding process of the large pdf dataset.

\begin{table}[H]
    \centering
    \caption{Hardware Requirements}
    \label{tab:hardware_requirements}
    \begin{tabular}{ll}
        \hline
        \textbf{Component}       & \textbf{Specification}                     \\ \hline
        Processor (CPU)          & Modern Multi-core CPU                      \\
        Memory (RAM)             & 16 GB or higher                            \\
        Storage                  & 1 TB SSD or higher                         \\
        Graphics Card (GPU)      & NVIDIA RTX 3090+ (recommended)             \\
        \hline
    \end{tabular}
\end{table}

The researchers considered using a modern multi-core CPU to enable efficient data processing and high inference.  Sufficient memory was also crucial; At least 16 GB of RAM is recommended and is the least to manage large-scale ingesting and embeddings. A 1 TB SSD was preferred to use due to its high read/write speeds, which help in the indexing process, and also serves as the data storage for thesis PDF documents. Considering also that LLM embeddings are resource-intensive when used, a powerful GPU like NVIDIA RTX 3090 or a higher version was recommended to speed up deep learning inference and vector operations. 

\subsubsection{Software}

\begin{table}[H]
    \centering
    \caption{Software Requirements}
    \label{tab:software_requirements}
    \begin{tabular}{ll}
        \hline
        \textbf{Component}      & \textbf{Specification}                                                               \\ \hline
        Programming Language    & Python 3.10+                                                                         \\
        Vector Database         & FAISS                                                                                \\
        Language Model          & Gemini 2.5-flash                                                                     \\
        Embedding Model         & \begin{tabular}[c]{@{}l@{}}sentence-transformers/\\ all-MiniLM-L6-v2 (HuggingFace)\end{tabular} \\
        Web Framework           & Flask                                                                            \\
        Libraries               & \begin{tabular}[c]{@{}l@{}}LangChain \\ PyMuPDF \\ NumPy \end{tabular} \\
        \hline
    \end{tabular}
\end{table}

Python serves as the core programming language due to its strong support for ML and NLP. While version 3.10 or later was used as the latest stable version, this may vary if there's a new update. FAISS from Facebook was utilized as the vector database for its fast vector similarity search and compression. The Gemini 2.5-flash LLM called from Google Generative AI API serves as the reasoning model for the chatbot, and the sentence-transformers/all-MiniLM-L6-v2 from Hugging Face was used as the embedding model that transformed chunks into vector embeddings. While the Flask framework was used to build the whole system for its simplicity.

Various modules was also called to help in document parsing and extraction. One is through PyMuPDF for text extraction from thesis PDF files. NumPy for numerical operations and help a lot in the evaluation, and the LangChain framework manage the orchestration of LLMs during query interpretation and response generation. 


\subsection{Instrument}

In this subsection, the instruments that was used by researchers to analyze and evaluate the performance of the RAG chatbot system.

\textit{RAGAS (Retrieval-Augmented Generation Assessment Suite)}. RAGAS is a framework for reference-free evaluation of RAG pipelines. This toolkit was used to automate evaluation of the quality of system outputs using its metrics such as context precision, faithfulness, and answer relevance \cite{shinn2023ragas}. Furthermore, a context recall metric was included, as recommended for evaluating retrieved chunks.

\textit{Survey.} Instruments served as data collection tools across different areas and provided an effective way to gather information. They were useful when seeking insights into the attributes, preferences, opinions, or beliefs of a specific group. To meet the study objectives, the researchers conducted a survey among CSPC librarians and students to evaluate the proposed RAG chatbot. Using a user-centered method that measured users’ level of agreement on the chatbot’s quality and performance, the researchers created a questionnaire to assess users’ satisfaction with answers, likelihood to use the chatbot again, ease of reading and understanding the output, and confidence in the information retrieved by the system. There are 100 respondents in the study from the CSPC who served as representatives of the whole population.


\subsection{Statistical Test}

The RAG system performance was evaluated using the RAGAS framework, focusing on context precision, recall, relevance, and faithfulness to measure how well relevant documents were retrieved and responses generated \cite{holmes2023chatbot, ameli2024ranking, lin2024satisfaction}. Additionally, to assess not only the technical but also the user-centered performance of the system, a user questionnaire was administered to collect feedback regarding usability, accuracy, and overall satisfaction.

The Likert Scale, introduced by \citeauthor{likert1932technique} [\citeyear{likert1932technique}], is a measurement method for evaluating individuals' attitudes toward any object. % It indicates the degree to which they agree or disagree about the issue. 
In particular, the 5-point Likert Scale was chosen because it works well in surveys and requires less time and effort to develop \cite{likert1932technique, rukundo}.

% The Likert Scale, introduced by \citeauthor{likert1932technique} \citeyear{likert1932technique}, is a measurement method developed for evaluating individuals' attitudes toward any object. % It indicates the degree to which they agree or disagree about the issue. In particular, the 5-point Likert Scale was chosen because it works well in surveys and requires less time and effort to develop \cite{likert1932technique, rukundo}.

\begin{table}[H] %TABLE 3%
    \centering
    \caption{Likert Scale for User Level of Agreement}
    \label{tab:likert_scale}
    \footnotesize
    \begin{tabular}{cccp{7cm}}
        \hline
        \textbf{Scale} & \textbf{Range} & \textbf{Level of Agreement} & \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
                5 & 4.21 - 5.00 & Strongly Agree & The participant strongly supports or agrees with the chatbot's response.\\
        \hline
                4 & 3.21 - 4.20 & Agree & Implies a positive stance toward the chatbot's response. \\
        \hline
                3 & 2.61 - 3.20 & Neutral & The respondent has neither a positive response nor a negative response, but undecided denotes a state of confusion of the respondent. \\
        \hline
                2 & 1.81 - 2.60 & Disagree & Suggests a level of disagreement with the statement or question, but not as strong as Strongly Disagree.\\
        \hline
                1 & 1.00 - 1.80 & Strongly Disagree & Indicates a strong and definitive disagreement with the statement or question. The respondent strongly opposes or disagrees with the chatbot's response.\\
        \hline
    \end{tabular}
\end{table}

\ref{tab:likert_scale} shows that this study employed a 5-point Likert Scale to determine "User level of Agreement" based on the user’s experience with the system’s response quality and performance. In the table, the first column showed the scale, and the second is its equivalent range, then the scale that the system level of agreement fell under was shown in the third column, and with its corresponding description in the fourth column. The user’s response was computed using the weighted mean statistical method, and then it was determined in which range it fell.  

The scale is the numerical value that corresponds to the level of agreement ( Strongly agree, agree, neutral, disagree, strongly disagree). This is important as it allows it to be used in statistical analysis. Scale 5 with a range of 4.20 - 5.00 described as “Strongly Agree”, which means that the users who tested the chatbot completely agreed with the described criteria or considered its quality and performance excellent. Scale 4 with a range 3.40 - 4.19 described as “Agree”, shows that the user who tested the chatbot generally agrees with the described criteria, but not to the strongest extent. The Scale 3 with a range of 2.60 - 3.39 was described as “Neutral”, meaning that the user of who tested the chatbot is neutral, undecided, or the criteria description doesn’t strongly resonate in either direction. Scale 2 with a range of 1.80 - 2.59 described as “Disagree”, denoting that the user who tested the chatbot disagrees with the criteria description but not as intensely as “Strongly Disagree”. And lastly,  Scale 1 was described as “Strongly Disagree”, meaning that the user who tested the chatbot completely disagrees with the criteria description or finds the system to be low quality and low performance.  

Another set of questionnaires was prepared to determine the level of user satisfaction, likelihood of using the application in the future, ease of reading and understanding chatbot output, and confidence in response information regarding the RAG chatbot system. User opinions are expected to provide an understanding of the overall user experience and satisfaction level with the chatbot’s performance in thesis retrieval tasks and its accessibility that proposed to solve the problem. A Likert scale-type questionnaire was also used for this purpose, employing the same 5-point scale framework to ensure consistency in measurement and analysis as shown in \ref{tab:satisfaction_scale}.

% Another set of questionnaires will be prepared to determine the level of user satisfaction, likelihood of using the application in the future, ease of reading and understanding chatbot output, and confidence in response information regarding the RAG chatbot system. User opinions are expected to provide an understanding of the overall user experience and satisfaction level with the chatbot's performance in thesis retrieval tasks. A Likert scale type of questionnaire will also be used for this purpose, employing the same 5-point scale framework to ensure consistency in measurement and analysis.

\begin{table}[H] %TABLE User level satisfaction%
    \centering
    \caption{User Level of Satisfaction with Answers}
    \label{tab:satisfaction_scale}
    \footnotesize
    \begin{tabular}{cccp{7cm}}
        \hline
        \textbf{Scale} & \textbf{Range} & \textbf{Level of Satisfaction} & \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
        5 & 4.21 - 5.00 & Very Satisfied & The participant is very satisfied with the chatbot's answers.\\
        \hline
        4 & 3.21 - 4.20 & Satisfied & Indicates a positive satisfaction toward the chatbot's answers. \\
        \hline
        3 & 2.61 - 3.20 & Neutral & The respondent has neither a positive nor negative satisfaction; undecided or indifferent denotes a state of uncertainty of the respondent. \\
        \hline
        2 & 1.81 - 2.60 & Unsatisfied & Suggests a level of dissatisfaction with the chatbot's answers, but not as strong as Very Unsatisfied.\\
        \hline
        1 & 1.00 - 1.80 & Very Unsatisfied & Indicates a strong and definitive dissatisfaction with the chatbot's answers. The respondent is very unhappy with the chatbot's responses.\\
        \hline
    \end{tabular}
\end{table}

\ref{tab:satisfaction_scale} shows that RAG chatbot system used 5-point Likert Scale to determine users Level of Satisfaction based on the user’s experience to the system’s answers. The first column showed the scale that the system level of satisfaction fell under which was shown in third column and its corresponding definition in the fourth column. User’s response would be computed using weighted mean and was determined in which range fell under. Scale 5 with a range of 4.20-5.00 described as “Very Satisfied” which means that the user of the RAG Chatbot completely satisfies with the provided answers, scale 4 with a range 3.40-4.19 described as “Satisfied”, the user of the RAG Chatbot is satisfied with the provided answers but not to the strongest extent, scale 3 with a range 2.60-3.39 described as “Neutral”, the user of the RAG chatbot is neutral, undecided, or the criteria description doesn’t strongly resonate in either direction, Scale 2 with a range of 1.80-2.59 described as “Unsatisfied”, the user of the RAG Chatbot is unsatisfied with the provided answers but not as intensely as “Very Unsatisfied”, and Scale 1 is described as “Very Unsatisfied”, the user of the RAG chatbot completely dissatisfied with the provided answers.

\begin{table}[H] %Table for Scale Using the chatbot again%
    \centering
    \caption{Likert Scale for User Level of Using the Chatbot Again}
    \label{tab:reuse_scale}
    \footnotesize
    \begin{tabular}{cccp{5cm}}
        \hline
        \textbf{Scale} & \textbf{Range} & \textbf{Level of Using the Chatbot Again} & \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
        5 & 4.21 - 5.00 & Very Likely & The participant is very likely to use the chatbot again.\\
        \hline
        4 & 3.21 - 4.20 & Likely & Implies a positive intention to reuse the chatbot.\\
        \hline
        3 & 2.61 - 3.20 & Neutral & The respondent is undecided or indifferent about using the chatbot again.\\
        \hline
        2 & 1.81 - 2.60 & Unlikely & Suggests a low intention to reuse the chatbot, but not as strong as Very Unlikely.\\
        \hline
        1 & 1.00 - 1.80 & Very Unlikely & Indicates a strong and definitive intention not to use the chatbot again. The respondent is very unlikely to reuse the chatbot.\\
        \hline
    \end{tabular}
\end{table}

\ref{tab:reuse_scale} shows that RAG chatbot system used 5-point Likert Scale to determine users Level of Using the Chatbot Again based on the user’s intention to reuse the system after their experience. The first column showed the scale that the system level of reuse fell under which was shown in third column and its corresponding definition in the fourth column. User’s response would be computed using weighted mean and will be determined in which range fell under. Scale 5 with a range of 4.20-5.00 described as “Very Likely” which means that the user of the RAG Chatbot is very likely to use the system again or finds it highly useful, scale 4 with a range 3.40-4.19 described as “Likely”, the user of the RAG Chatbot is likely to use the system again but not to the strongest extent, scale 3 with a range 2.60-3.39 described as “Neutral”, the user of the RAG chatbot is neutral, undecided, or the criteria description doesn’t strongly resonate in either direction, Scale 2 with a range of 1.80-2.59 described as “Unlikely”, the user of the RAG Chatbot is unlikely to use the system again but not as intensely as “Very Unlikely”, and Scale 1 is described as “Very Unlikely”, the user of the RAG chatbot is very unlikely to reuse the system or finds it not useful enough to return.

\begin{table}[H] %TABLE chatbot response user level%
    \centering
    \caption{User Level of Understanding Chatbot Responses}
    \label{tab:understanding_scale}
    \footnotesize
    \begin{tabular}{cccp{7cm}}
        \hline
        \textbf{Scale} & \textbf{Range} & \textbf{Level of Understanding} & \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
        5 & 4.21 - 5.00 & Very Easy & The participant finds the chatbot's responses very easy to understand.\\
        \hline
        4 & 3.21 - 4.20 & Easy & Implies generally easy comprehension of the chatbot's responses.\\
        \hline
        3 & 2.61 - 3.20 & Neutral & The respondent neither finds the responses easy nor difficult; undecided denotes a state of ambivalence.\\
        \hline
        2 & 1.81 - 2.60 & Difficult & Suggests some difficulty in understanding the chatbot's responses, but not as severe as Very Difficult.\\
        \hline
        1 & 1.00 - 1.80 & Very Difficult & Indicates the participant finds the chatbot's responses very difficult to understand.\\
        \hline
    \end{tabular}
\end{table}

\ref{tab:understanding_scale} shows that RAG chatbot system will use 5-point Likert Scale to determine users Level of Understanding based on the user’s ease in reading and comprehending the system’s responses. The first column showed the scale that the system level of understanding fell under which was shown in third column and its corresponding definition in the fourth column. User’s response would be computed using weighted mean and will be determined in which range fell under. Scale 5 with a range of 4.20-5.00 described as “Very Easy” which means that the user of the RAG Chatbot finds the chatbot’s responses very easy to understand or finds its clarity and readability excellent, scale 4 with a range 3.40-4.19 described as “Easy”, the user of the RAG Chatbot finds the responses easy to understand but not to the strongest extent, scale 3 with a range 2.60-3.39 described as “Neutral”, the user of the RAG chatbot is neutral, undecided, or the criteria description doesn’t strongly resonate in either direction, Scale 2 with a range of 1.80-2.59 described as “Difficult”, the user of the RAG Chatbot finds the responses difficult to understand but not as intensely as “Very Difficult”, and Scale 1 is described as “Very Difficult”, the user of the RAG chatbot finds the responses very difficult to understand or considers the system unclear and hard to interpret.

\begin{table}[H] %TABLE 3%
    \centering
    \caption{Likert Scale for User Level of Confidence on Information Received}
    \label{tab:confidence_scale}
    \footnotesize
    \begin{tabular}{cccp{7cm}}
        \hline
        \textbf{Scale} & \textbf{Range} & \textbf{Level of Confidence} & \multicolumn{1}{c}{\textbf{Description}} \\
        \hline
        5 & 4.21 - 5.00 & Very Confident & The participant is very confident in the information received from the chatbot.\\
        \hline
        4 & 3.21 - 4.20 & Confident & Implies a general confidence in the chatbot's information.\\
        \hline
        3 & 2.61 - 3.20 & Neutral & The respondent neither expresses confidence nor distrust; undecided denotes a state of uncertainty of the respondent.\\
        \hline
        2 & 1.81 - 2.60 & Unconfident & Suggests some lack of confidence in the chatbot's information, but not as strong as Very Unconfident.\\
        \hline
        1 & 1.00 - 1.80 & Very Unconfident & Indicates a strong and definitive lack of confidence in the chatbot's information. The respondent is very unconfident about the chatbot's responses.\\
        \hline
    \end{tabular}
\end{table}

\ref{tab:confidence_scale} shows that RAG chatbot system used a 5-point Likert Scale to determine users Level of Confidence based on the user’s trust and perceived accuracy of the information retrieved by the system. The first column showed the scale that the system level of confidence was classified as, which was shown in third column and its corresponding description in the fourth column. User’s response would be computed using weighted mean and will be determined in which range fell under. Scale 5 with a range of 4.20-5.00 described as “Very Confident” which means that the user of the RAG Chatbot is very confident in the information received or finds its accuracy and reliability excellent, scale 4 with a range 3.40-4.19 described as “Confident”, the user of the RAG Chatbot is confident in the information but not to the strongest extent, scale 3 with a range 2.60 - 3.39 described as “Neutral”, the user of the RAG chatbot is neutral, undecided, or the criteria description doesn’t strongly resonate in either direction, Scale 2 with a range of 1.80-2.59 described as “Unconfident”, the user of the RAG Chatbot lacks confidence in the information retrieved but not as intensely as “Very Unconfident”, and Scale 1 is described as “Very Unconfident”, the user of the RAG chatbot is very unconfident in the chatbot's information or finds the responses unreliable.

In order to analyze the gathered data from the user evaluation questionnaire, the researchers employed the Weighted Mean as the statistical tool. This method was chosen for its effectiveness in summarizing data responses from the Likert scale, in where it can provide a detailed understanding of user perceptions regarding the chatbot's usability and performance. Also, the level of satisfaction with chatbot answers, likelihood of using the chatbot again in the future, ease of reading and understanding the output, and users’ confidence in the accuracy of the chatbot’s responses was  evaluated using this computation:


\begin{equation}
    \centering
    WM = \frac{TWM}{N}
\end{equation}

Where:
\begin{itemize}
    \item $WM$ = Weighted Mean
    \item $TWM$ = Total Weighted Mean
    \item $N$ = Total number of respondents
\end{itemize}

\section{Procedures}
The procedure includes the most important stages in building this project. Each step plays a role in addressing this project's objectives. 

% \subsection*{Data Collection}

% PDF thesis documents were gathered from CSPC Library’s digital archives, focusing on undergraduate theses and institutional research. The collection process ensured that documents were academically relevant and representative of typical user queries.

\begin{enumerate}

    \item \textbf{Data Preprocessing} - The collected PDF thesis documents underwent text extraction, cleaning, and chunking. This stage, along with the next, is done in a separate Python notebook to support visualizations.
        \begin{enumerate}
            \item [(a)] {Text Extraction:} The PyMuPDF was used to extract text from the PDF files. This is useful for converting the pdf to a structured Markdown language.
            \item [(b)] {Cleaning:} Data cleaning was employed since the thesis structure is somewhat messy and has redundant or non-informative characters and formatting.  One example is headers that appear on every page were extracted at first, but removed. 
            \item [(c)] {Text Chunking:} Since every thesis document is composed of hundreds of pages and thousands of text, these were divided into smaller, manageable chunks. 
        \end{enumerate}
        
    \item \textbf{Indexing and Vector Embedding} - In this stage, the preprocessed chunks were transformed into vector representations and indexed for efficient retrieval.
        \begin{enumerate}
            \item [(a)] {Vector Embedding:} Each text chunk was embedded using sentence-transformers/all-MiniLM-L6-v2 from Hugging Space.
            \item [(b)] {Data Storing:} FAISS then stored the vectors along with metadata such as document titles, authors, and section headers.
        \end{enumerate}

    \item \textbf{Query Handling and Semantic Retrieval} - User queries need to be understandable not just by human but also by computers. Also, it should be compatible with the stored vectors from the previous indexing. This step vectorized the user query using the same embedding model to be compared using similarity search in retrieving relevant chunks from the FAISS database.
        \begin{enumerate}
            \item [(a)] {Query Encoding:} The user query is transformed into a high-dimensional vector using the same embedding model applied in indexing. Guard rail was also applied to prevent unethical output of the system.latent space.
            \item [(b)] {Similarity Search:} That vectorized query then matched against stored vectors from the FAISS to retrieve the top-K relevant chunks, with default K  set to 50. This K also have a conditional parameter set in the code allowing it to adaptively increased to 100 for exhaustive searches.
        \end{enumerate}

    \item \textbf{Response Generation} - Gemini 2.5-flash language model effectively processed the augmented input to generate a human-like response. This choice supports multimodal language and low-latency response generation at a low cost.

    \item \textbf{Output Presentation} - Using the Flask web framework, the system displayed the generated response through a ChatGPT-styled user interface, which most of the time includes the title, summary, and URL of the thesis for full accessibility.


    \item \textbf{Performance Evaluation}
        \begin{enumerate}
            \item [(a)] {Automated Evaluation:} Metrics from the RAGAS framework such as Context Precision, Context Recall, Answer Relevance, and Faithfulness, were used to evaluate model performance. The RAGAS is one of the most used evaluation framework in RAG architectures due to its fast evaluation cycle \cite{es2024ragas}.
            \item [(b)] {Human Evaluation:} A usability questionnaire was distributed to a sample of student users to assess the system’s clarity, ease of use, and usefulness in retrieving academic information.
        \end{enumerate}

\end{enumerate}


\section{Evaluation Metrics}
Researchers evaluated the performance of the RAG-Based System using the RAGAS\cite{es2024ragas}, which focusing on context precision, context recall, response relevance, and faithfulness. With these metrics, it will evaluate retrieval accuracy, quality, query similarity, and factual grounding of generated answers, ensuring that system produces reliable and useful outputs for users. 

\subsection{Context Precision}

Context Precision metric was used to evaluate the retrieval value of the RAG chatbot within the CSPC Library. It measured the amount of relevant document chunks among the top $K$ retrieved results, that emphasizing the system’s ability to present highly relevant content at higher ranks. A higher Context Precision specify that the system effectively prioritized relevant information for the user. 

\begin{equation}
\centering
\text{Context Precision@K} = 
\frac{
    \sum_{k=1}^{K} \left( \text{Precision@k} \times v_k \right)
}{
    \text{Total number of relevant items in the top } K \text{ results}
}
\end{equation}

where $\text{Precision@k}$ denotes the precision at rank $k$, and $v_k$ is a binary indicator variable such that $v_k = 1$ if the chunk at position $k$ is relevant, and $v_k = 0$ otherwise. The $K$  indicates the cutoff for the top results evaluated, while the denominator was the one that normalizes the metric by considering the total number of relevant items within the top $K$ retrieved results. This weighted approach helped so that relevant items retrieved earlier in the ranking will contribute more to the final score, making the metric meaningful for library retrieval tasks. 

The precision at each position $k$, denoted as Precision@k, was calculated using this equation:

\begin{equation}
\centering
\text{Precision@k} = 
\frac{
    \text{true positives@k}
}{
    \text{true positives@k} + \text{false positives@k}
}
\end{equation}

where $\text{true positives@k}$ is the number of relevant chunks retrieved up to position $k$, while the  $\text{false positives@k}$ was the number of non-relevant chunks that was retrieved up to the same position. This metric calculates retrieval accuracy at each rank and serves as the main basis for the overall Context Precision@K calculation.

\subsection{Context Recall}

Context Recall was used to evaluate the comprehensiveness of the retrieval system in capturing all relevant information necessary to answer a query. In this study, It measured the amount of relevant chunks successfully retrieved by the RAG system within its knowledge base. 
% For this, a higher recall is bad as it indicates that fewer relevant chunks were left out. So in essence, recall is about not missing important chunks. 

\begin{equation}
\centering
\text{Context Recall} = \frac{\text{Number of relevant claims supported by retrieved chunks}}{\text{Total number of relevant claims in the reference answer}}
\end{equation}

where:

\begin{itemize}
    \item \textit{Number of relevant claims supported by retrieved chunks} refers to the count of factual claims in the ground truth answer that can be attributed to the retrieved chunks. 
    \item \textit{Total number of relevant claims in the reference answer} represents all the factual claims present in the ground truth answer that ideally should be covered by the retrieval process.
\end{itemize}

This metric show how effectively the system provides the necessary knowledge, with a value ranging between 0 and 1, where 1 specify the perfect recall. It ensures that important academic information is not missed during retrieval, making it an essential part of evaluating the RAG system.


\subsection{Response Relevance}

The response relevance was a crucial metric that was used to evaluate how relevant was the response RAG system in answering  the specific query asked by the user. This metric works by penalizing responses that are either incomplete or contains unnecessary detail. 

\begin{equation}
\centering
\text{Response Relevance} = \frac{1}{N} \sum_{i=1}^{N} \cos(E_{g_i}, E_o)
\end{equation}

where:
\begin{itemize}
    \item $N$ is the number of artificially generated questions based on the response (typically 3),
    \item $E_{g_i}$ is the embedding of the $i$-th generated question obtain from the response,
    \item $E_o$ is the embedding of the user query,
    \item $\cos(E_{g_i}, E_o)$ represents the cosine similarity between the generated question embedding and the original query embedding.
\end{itemize}

This metric was based on the idea that if the chatbot’s response correctly answered the original query, then questions generated from that response would semantically similar with the original question. This involved generating multiple artificial questions, and also embedding both the response-generated questions and the original query into vector embeddings. The mean cosine similarity was then calculated to measure the alignment, so that the retrieved academic information will closely match the research needs of the users.

\subsection{Faithfulness}

The Faithfulness is a critical metric for evaluating the consistency of the RAG chatbot’s that generates responses with the retrieved context from the CSPC Library. This metric was used so that all claims made in the chatbot’s answer are directly supported by the information present in the retrieved documents, thereby reducing hallucinations and maintaining academic integrity.

\begin{equation}
\centering
\text{Faithfulness} = \frac{\text{Number of claims in the response supported by retrieved context}}{\text{Total number of claims in the response}}
\end{equation}

where:
\begin{itemize}
\item \textit{Number of claims in the response supported by retrieved context} refers to the count of factual statements in the generated answer that can be directly verified or inferred from the retrieved context chunks.
\item \textit{Total number of claims in the response} is the complete count of all factual statements made in the answer, even so whether they are supported by the context. A faithfulness score of 1.0 shows that all claims in the response are grounded in the retrieved context, while lower scores reveal the presence of unsupported or hallucinated.
\end{itemize}

A faithfulness score of $1.0$ indicates that all claims in the response are grounded in the retrieved context, while a lower score means that hallucination has occurred. In this study, a high faithfulness result is crucial so that the RAG system's generated answers are factually correct and grounded in the thesis chunks.

\section{Conceptual Framework}

This section presents the conceptual framework adapted in the study. This serves as the foundational blueprint for the RAG chatbot. As illustrated in Figure 2, the system followed a cyclical process starting from data collection and ending with performance evaluation. The arrows were used to indicate the step-by-step flow of each component within the framework; note that these did not signify any technical operation or special relationship beyond showing the direction of the process. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/framework.png}
    \caption{Conceptual Framework of the RAG-Based Chatbot System}
    \label{fig:conceptual_framework}
\end{figure}

\textit{Data Collection}. The process began when the researchers started coordinating with CSPC Library, where the prototype was demonstrated to show how RAG chatbot could improve searching, specifically in thesis retrieval. In the demonstration, the project’s institutional value was emphasized in revolutionizing access to information, as well as addressing the pain points for the researchers. Following that was the researchers’ formal request to gain access to all available thesis PDFs from various College departments of CSPC to use as the main corpus of the proposed RAG system.

\textit{Data Pre-processing}. PyMuPDF was used to extract text from the collected PDFs and then convert it to markdown. The extracted content underwent data cleaning to remove redundant and non-informative characters, and was followed by a token-based chunking strategy.

\textit{Indexing and Vector Database Construction}. Each chunk was embedded using the sentence-transformers/all-MiniLM-L6-v2 model from Hugging Face, converting it into dense vectors that capture semantic meanings. These vectors were stored in a FAISS vector database for fast and efficient similarity search. Moreover, metadata was also stored for each document. This construction was research-proven, surpassing traditional keyword-based searching. 

\textit{Query Encoding and Retrieval}. In this part, when the user started to query, those queries were encoded into dense vectors using the same embedding model used in indexing. The system then performed a similarity search in FAISS, retrieving the top-K relevant chunks. 

\textit{Augmented Input Generation}. In this study, the augmented input generation phase served as the bridge between retrieved thesis content and response generation. This is where the retrieved top-k chunks are combined with the user query to guide the reasoning model in generating an answer.

\textit{Response Generation}, This stage is where the reasoning model takes its action. For this, Gemini 2.5 flash generates a tailored response based on the augmented input. Choosing this model enables a late latency response and multimodal support that can even understand the local language of CSPC students.  

\textit{Response Output and Interface Display}. Web frameworks such as Flask and Django are commonly used to develop chatbot web applications. These frameworks provide complete tools and libraries for building web applications. In this project, Flask was preferred due to its simplicity, and the Langchain library was also supported in the web construction. Moreover, user authentication and access control for regular users and admin was applied.

\textit{Performance Evaluation}. In the last part, the system’s effectiveness was evaluated with two approaches: RAGAS metrics (context precision, context recall, answer relevance, faithfulness) and a user-centered method that measured users’ level of agreement on the chatbot’s quality and performance.


%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}
