\chapter{Summary of Findings, Conclusions, and Recommendations}
\begin{refsection}

This chapter presents the summary of findings, conclusions, and recommendations derived from the results of the study.

\section{Summary}

Finding a relevant thesis in university library, like in CSPC, can be difficult for many students and researchers. Most of them have a hard time finding the exact thesis they need because the current library website only allows searches by exact title or keywords. If a user does not know the exact title, it becomes a struggle to locate the right documents. And making things even harder, library rules do not allow any thesis to be borrowed, which means users must visit the library in person to access it. Because of these challenges, this study explored creating a conversational chatbot that would let users search for thesis papers using topics, keywords, or even general descriptions, all while making the system accessible everywhere.

To solve these problems, the researchers built a new chatbot system that uses RAG along with a state-of-the-art LLM. The process involved preprocessing and converting 290+ undergraduate thesis papers into vector embeddings and storing them in a FAISS vector database, which allows the chatbot to understand and search for relevant information based on a user’s questions in natural language. The chatbot retrieves and displays the most fitting parts of the theses and uses the Gemini 2.5-flash model to generate fast, accurate, and appropriate responses that support local language. All the system steps, from collecting the thesis pdf files to designing a simple user interface in Flask, have worked together to make searching faster and truly effective. Also, the chatbot was deployed in the cloud, so anyone can access it whenever they are. Lastly, the system was tested using different metrics from the RAGAS evaluation framework, including context precision, context recall, answer relevance, and faithfulness. In addition to those, a user questionnaire with a 5-point Likert scale was used to assess user-centered performance.

The results are promising, both from the RAGAS system evaluation and the user-centered survey. Context Precision (0.9167) and Faithfulness (0.9179) have demonstrated strong competence in retrieving relevant content or chunks while reducing the likelihood of hallucinations. Context Recall reached 0.8711, still shows a successful thesis retrieval despite minor gaps. Answer Relevancy scored 0.8625, which was interpreted that the generated responses had effectively addressed user queries. In the survey questionnaire, interviewees strongly agreed with a weighted mean of 4.5 on the "Chatbot’s Overall Response Quality and Performance," which means that the chatbot works well for its main goal of helping students and researchers find relevant theses in a conversational way. Moreover, regarding the "RAG Chatbot’s Overall Effectiveness and Usability," a weighted mean of 4.3, which indicates that the majority have strongly agreed that the chatbot provides useful and relevant answers, showing intent to use the chatbot again in the future, to support their academic needs. However, both evaluations reveal important dependencies: chunk quality, OCR support, and prompt variations remain as the optimization target. Despite these limitations, overall performance has demonstrated the chatbot's effectiveness in revolutionizing thesis retrieval and literature search.

\section{Findings}

Below are the key findings in this study:

\begin{enumerate}
    \item By integrating a document ingestion and retrieval module as the initial objective, all thesis documents in PDF format were standardized and divided into meaningful chunks. It was then enriched with metadata, embedded, and indexed in FAISS. This makes them discoverable through semantic search, resulting in much better retrieval of abstracts, authors, chapters, etc. The chunking strategy and metadata tagging (such as abstract, methodology, or results) have also improved context alignment and ranking, so queries like “give me the complete abstract” or “find theses related to nursing” return the exact relevant sections. In contrast, traditional keyword-based search usually requires exact titles or a strict keyword search.

    % \item By implementing a semantic search and thesis retrieval system with RAG orchestrated in LangChain, FAISS as the vector database, using all-MiniLM-L6-v2 from HuggingFace for vector embeddings and Google Gemini 2.5-flash as the generative LLM, user queries are matched to semantically relevant thesis chunks, which consistently yield more precise answers to intent-driven questions. The Flask web framework was used to build the chatbot system, which enabled fast development due to its simplicity and Python library support. The created chatbot as a thesis retrieval system,  when deployed to the cloud, has made the system more accessible to all users, solving the issue of the need to visit the library onsite.

    \item By implementing a semantic search and thesis retrieval system with RAG orchestrated in LangChain, FAISS as the vector database, using all-MiniLM-L6-v2 from HuggingFace for vector embeddings, and Google Gemini 2.5-flash as the reasoning model, user queries are matched to semantically relevant thesis chunks, which consistently return fast and more accurate answers to intent-driven questions. The Flask web framework was used to build the whole chatbot system, including the authentication, role-based access control, and user-friendly interface. This enabled fast development due to its simplicity and Python library support. Then, the created chatbot as a thesis retrieval system,  when deployed to the cloud, has made the system more accessible to all users, solving the issue of the need to visit the library onsite.

    \item Using RAGAS as the automated evaluation and survey questionnaires for human evaluation, Context Precision reaches 0.9167, Context Recall of 0.8711, Answer Relevancy of 0.8625, and Faithfulness of 0.9179 indicates a strong retrieval-and-generation pipeline: high precision and recall show that the FAISS-backed retrieval returns the necessary thesis passages, while the strong answer relevancy demonstrates that the LLM composes useful responses from retrieved sources. The high faithfulness score suggests that generated outputs are, in most cases, directly grounded in retrieved documents, which materially reduces the likelihood of hallucinations. In the human evaluation, the chatbot achieved a Strongly Agree with a weighted mean of 4.5 rating for the overall response quality and performance, as well as a Strongly Agree with a weighted mean of 4.3 rating for the overall effectiveness and usability of the chatbot. However, both evaluations still highlight the usual dependencies and limitations, chiefly the chunk quality from the ingested thesis PDFs, OCR support, and user prompt variations, which remain important targets for continued optimization even as overall performance is strong.

\end{enumerate}

\section{Conclusions}

    Based on the findings, the researchers come up with the following conclusions:

\begin{enumerate}
    \item It was evident that the document ingestion is a critical first step in making this project. By preprocessing documents, applying semantic chunking, and embedding, the system overcomes the limitations of traditional keyword-based search. This first objective makes the extracted context stored in the vector database ready to be more accessible through semantic search. 

    \item By implementing a semantic search and thesis retrieval system using RAG orchestrated in LangChain, FAISS for approximate nearest-neighbor search, using sentence-transformers/all-MiniLM-L6-v2 from HuggingFace for vector embeddings and Google Gemini 2.5-flash as the reasoning model, user queries are matched to semantically relevant chunks, which gives precise answers to general users' queries compared to the current keyword-based search.  By combining semantic search with Google Gemini 2.5-flash, the chatbot has an advanced reasoning capability at a low latency and multimodal support.

    \item The system evaluation proves that the RAG system effectively retrieves the most relevant chunks, and its generated answers were appropriate. However, minor hallucinations were also observed, so it's important that users still verify the answer provided by viewing the URLs the chatbot gave, for review. 
\end{enumerate}

\section{Recommendations}
Based on the conclusions, the researchers recommend the following:

\begin{enumerate}
    \item Future work should focus on re-enhancing the ingestion process by exploring other modern chunking methods to further improve the context precision, recall metrics result.

    \item Also, explore the digitalization of all academic materials like books in the library improve the retrieval coverage of the chatbot.

    \item It is recommended to adapt to the latest and most useful features of well-known chatbots like ChatGPT. Further development of the UI, including the citation export and personalized recommendations, can help with the user experience.
\end{enumerate}

%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}