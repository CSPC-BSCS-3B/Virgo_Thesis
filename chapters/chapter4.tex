
\chapter{Results and Discussion}
\begin{refsection}

In this chapter, the detailed development process, results, and evaluation of the Retrieval-Augmented Generation (RAG) chatbot developed for efficient literature search and thesis retrieval at the Camarines Sur Polytechnic Colleges (CSPC) Library are presented. The chapter outlines the study’s objectives, stated in Chapter 1. Each section had discussed the findings and their implications, framed under the RAG-based structure.

\section{Development of the RAG Chatbot}
The RAG chatbot was developed using a combination of open-source tools and libraries, including LangChain, PyMuPDF, and a vector database for efficient document retrieval. The system architecture consists of three main components: the Document Ingestion and Retrieval Module, the Language Model Integration Module, and the User Interface Module.

\section{Document Ingestion and Retrieval Module}
The Document Ingestion and Retrieval Module successfully ingested a total of 100+ undergraduate and graduate thesis PDFs from various CSPC departments. After preprocessing, these documents were converted into thousands of structured text segments, each retaining metadata such as page numbers, headings, and file sources. This ensured that the academic integrity of the documents was preserved while making them machine-readable. Before the document ingestion process, the researchers first collected the data from the CSPC Library. A formal request letter was submitted to the CSPC Director to secure permission to access the thesis PDFs stored in the library’s repository. Once permission was granted, the documents were gathered and prepared for ingestion into the system.
Data Collection 
The researchers began with their proposal and coordination with CSPC Library and its staff, where the prototype was demonstrated to show how a RAG-powered chatbot could improve thesis discovery beyond exact-keyword search by enabling topic-oriented, semantically grounded retrieval within the library’s own repository. 
In the demonstration, the project’s institutional value was emphasized in accelerating literature searches, increasing access to relevant local theses, and supporting academic guidance, following the researchers' formal request to obtain one hundred undergraduate thesis PDFs from various College departments of CSPC to use as the main corpus of the RAG chatbot application.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{CSPC Undergraduate Thesis Documents}
\end{figure}
Upon agreement on project scope, data handling practices, and local/on-premise deployment, library personnel granted the researchers access to the digital copies of undergraduate thesis papers. The dataset was acknowledged and queued for the development phase following the proposed model pipelines’ process.
\subsection{Data Preprocessing}
In the \textbf{data pre-processing} phase, the systematic ingestion of the acquired thesis PDFs into the processing pipeline. Utilizing a custom script, the system recursively scanned the designated data folder housing more than 100 undergraduate thesis documents, ensuring all relevant files were accessed regardless of folder organization. Each PDF was processed page-by-page using the PyPDFLoader, extracting raw textual content alongside essential metadata such as source file paths and page numbers. This granular extraction maintained the academic formatting and pagination critical for preserving context, citations, and facilitating precise retrieval during later stages.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{Data Preprocessing inside the code editor}
\end{figure}
Extracted texts were segmented with a token-aware strategy (RecursiveCharacterTextSplitter), producing semantically coherent chunks sized to the LLM context window and guided by structural delimiters such as paragraph breaks and headings. Each chunk retained source, page, and positional offsets to support traceability and user navigation. The preprocessing yielded clean, contextually rich text segments ready for vectorization and indexing in the subsequent pipeline stage.
Next, the system will perform \textbf{model training}, where embedding models such as gemini embeddings will be used to transform the text chunks into vector representations.

\subsection{Indexing and Vector Database Construction}
In the \textbf{indexing and vector database construction} phase, the preprocessed text chunks were embedded into high-dimensional vector representations using state-of-the-art embedding models, such as Gemini Embeddings. Each chunk, enriched with metadata (source, page, and position), was passed through the embedding model to generate a unique vector capturing its semantic meaning.

These vectors were then stored in a vector database (e.g., FAISS or ChromaDB), enabling efficient similarity search and retrieval. The database indexed not only the vectorized content but also maintained links to the original metadata, ensuring that retrieved results could be traced back to their exact location in the source documents.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{Indexing and Vector Database Construction}
\end{figure}

Each text chunk underwent embedding generation through the GoogleGenerativeAIEmbeddings model, which converted the semantic meaning of academic content into dense numerical vectors. This embedding process captured nuanced relationships between concepts, methodologies, and findings across the diverse corpus of CSPC thesis documents. The resulting vectors preserved both explicit textual information and implicit contextual relationships, enabling the system to understand queries about similar research topics, methodological approaches, or theoretical frameworks even when exact keywords differed. Following embedding generation, the vector representations were systematically indexed and stored within FAISS, a specialized library engineered for high-performance similarity search operations on dense vectors. The database construction process organized vectors using efficient indexing algorithms that would later enable rapid retrieval of contextually relevant thesis segments. Metadata preservation ensured that each vector maintained its connection to source documents, page numbers, and positional information, creating a comprehensive mapping between semantic concepts and their original academic contexts. This FAISS-based indexing architecture formed the retrieval foundation that would allow users to discover relevant thesis content through natural language queries, moving beyond traditional keyword matching to genuine semantic understanding of academic literature


\subsection{Language Model Integration Module}
The Language Model Integration Module successfully integrated the Google Gemini large language model with the RAG framework. This integration enabled the chatbot to understand user queries in natural language and generate contextually relevant responses based on the retrieved thesis documents. The RAG framework combined retrieval and generation techniques, allowing the chatbot to provide accurate and informative answers to user queries.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{RAG Architecture}
\end{figure}
The RAG architecture was implemented using the LangChain framework, which facilitated the seamless integration of the vector database and the language model. The system was designed to first retrieve relevant text chunks from the vector database based on the user query, and then use the language model to generate a coherent response that incorporated the retrieved information. This approach ensured that the chatbot could provide accurate and contextually appropriate answers, enhancing the overall user experience.

\section{Semantic Search and Thesis Document Retrieval System}
The implementation of the semantic search and retrieval component allowed the chatbot to surpass traditional keyword-based searching by interpreting the underlying intent of user queries. Through the integration of vector embeddings and the FAISS index, the system consistently retrieved thesis passages that were contextually relevant, even when the phrasing of queries differed from the original text. The initial stage in this process involved generating augmented input, where the most relevant thesis segments were selected and prepared as an enriched context for response formulation.
This was achieved by querying the FAISS index with the user’s natural language input, retrieving top-matching text chunks based on semantic similarity. The retrieved segments were then concatenated to form a comprehensive context that encapsulated the key themes and findings related to the query. In the subsequent response generation phase, the Google Gemini large language model utilized this augmented context to produce coherent and informative answers. By grounding its responses in the retrieved thesis content, the model was able to generate replies that were not only relevant but also enriched with specific academic insights. This dual-stage approach of retrieval followed by generation ensured that users received answers that were both accurate and contextually appropriate, significantly enhancing the effectiveness of literature search and thesis retrieval within the CSPC Library.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{Semantic Search and Thesis Document Retrieval System}
\end{figure}

\subsection{Augmented Input Generation}
The augmented input generation phase served as the crucial bridge between retrieved thesis content and intelligent response formulation. Following successful retrieval of the top-$K$ relevant thesis segments, the system executed a concatenation process that preserved both content integrity and source attribution. Each retrieved document chunk underwent formatting that maintained its semantic value while incorporating lightweight citation markers such as ``[S1 thesis\_title.pdf p.15]'', ensuring academic provenance remained traceable throughout the response generation process.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{Augmented Input Generation}
\end{figure}

The augmented prompt construction represented the culmination of the retrieval pipeline, where user queries and retrieved contexts merged into comprehensive input structures designed to optimize LLM performance within academic constraints. Structured prompt templates orchestrated a dual-component input framework consisting of system-level instructions (establishing academic rigor expectations) and human-readable messages (combining the original user question with the formatted context string). Safeguards such as token limit monitoring, intelligent truncation strategies, and ``no context available'' indicators maintained input quality and prevented downstream errors.

\subsection{Query Encoding and Retrieval}

The query encoding and retrieval phase transformed user queries into vector representations compatible with the FAISS index. User queries such as ``What research has been done on machine learning applications in healthcare?'' were transformed into dense vectors using the same \texttt{GoogleGenerativeAIEmbeddings} model employed during the indexing phase. This ensured semantic consistency between query representation and stored document vectors. The retrieval mechanism then leveraged FAISS to compute cosine similarity, identifying the top-$K$ most relevant thesis segments. These chunks formed the contextual foundation that guided the language model's response generation, enabling retrieval even when user terminology differed from the original text.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{Query Encoding and Retrieval}
\end{figure}   

\subsection{Response Generation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{Response Generation}
\end{figure}

The response generation stage marked the culmination of the RAG pipeline, where \texttt{gemini-1.5-flash} transformed augmented academic context into coherent, factually grounded answers. Configured with \texttt{temperature=0}, the model produced deterministic and reproducible responses, a requirement for academic applications. The model synthesized information from multiple thesis documents while adhering strictly to the retrieved content, reducing hallucination risks and ensuring academic rigor.

\section{Response Output and Interface Display}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/sampleFig1.jpg}
    \caption{Response Output and Interface Display}
\end{figure}

The final stage materialized through a Streamlit-based interface, enabling intuitive interaction with the chatbot. Queries were validated for appropriateness before being processed. Retrieved responses were displayed as Markdown-formatted text with citations, supporting academic workflows. Conversation history was preserved in the session state, enabling iterative inquiry and continuity. In cases of inappropriate queries, the system provided transparent warnings, ensuring responsible AI deployment within the CSPC academic environment.


%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}