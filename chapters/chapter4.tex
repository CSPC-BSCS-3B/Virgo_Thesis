\chapter{Results and Discussion}
\begin{refsection}

This chapter discusses the results and evaluation of the RAG chatbot developed for efficient literature search and thesis retrieval at the CSPC Library.

\section{Document Ingestion and Retrieval Module}

This implementation addressed the first specific objective of the study by transforming the library's static collection of thesis PDFs into a dynamic, searchable knowledge base. 

\subsection{Dataset and Preparation}
The study corpus comprised all available undergraduate thesis PDFs from multiple CSPC departments (290+ documents). The dataset was prepared via structured text extraction and token-based chunking aligned with thesis sections (Abstract; Chapters 1-5).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/dataset_sample.jpg}
    \caption{CSPC Thesis PDF Sample}
    \label{fig:dataset_sample}
\end{figure}

% Upon agreement on project scope and data handling, library personnel granted the researchers to gain access to the digital copies of undergraduate thesis papers. This composes of theses from different departments.

Upon agreement on project scope and data handling, library personnel allowed the researchers to access digital copies of undergraduate theses available.

\subsection{Data Preprocessing} Texts were extracted page by page and augmented with metadata to preserve scholarly provenance. Token based chunking generated coherent segments aligned with the Large Language Model context window, guided by the structure of the theses, improving the fidelity in retrieval and make citations transparent.

\begin{table}[H]
    \centering
    \caption{Chunk Analysis \& Statistics}
    \label{tab:chunk_analysis_stats}
    \begin{tabular}{ll} 
        \hline
        \textbf{Metric}            & \textbf{Value} \\
        \hline
        Total Chunks               & 38,127 \\
        Total Tokens               & 11,849,783 \\
        Avg Characters/Chunk       & 1323 \\
        Avg Words/Chunk            & 180 \\
        Minimum Tokens per chunk   & 124 \\
        Maximum Tokens  per chunk  & 1200 \\
        Median Tokens per chunk    & 335 \\
        \hline
    \end{tabular}
\end{table}

As putted in \ref{tab:chunk_analysis_stats}, the overall findings of the chunk analysis is seen and is that the implemented chunking framework is appropriately designed to handle preprocessing of document. In the analysis result found a total of 38,127 chunks equating to 11,849,783 tokens, with each chunk containing an average of 1,323 characters or approximately 180 words, and a median of 335 tokens per chunk. The provided results demonstrate that the generated chunks keeps within an appropriate size range to preserve semantic context while remaining suitable for vector embedding and retrieval.

In the case of chunk size control, the results indicate that the system indeed enforced the predefined limits. While there was a minimum chunk size of 124 tokens in order to avoid fragmented chunks or chunks with little information, which would affect quality in embedding, the maximum chunk size was set not to exceed 1,200 tokens to avoid notably large chunks, which might dilute semantic relevance.

\subsection{Indexing and Vector Database Construction}
In the phase of indexing preprocessed the text segments from the theses documents into a knowledge base optimized for semantic retrieval within the RAG framework. This stage is essentially crucial to link raw text materials with intelligent query response capabilities for the chatbot, underpinning its effectiveness in academic literature discovery.

Embeddings were created mainly with the help of sentence-transformers/all-MiniLM-L6-v2, taken from HuggingFace, which offers a good balance between efficiency and strong semantic performance. FAISS was used to store vectors along with page metadata to keep track of origin. In this type of scene, natural language queries could query semantically relevant segments of the thesis aside from exact keyword matches.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures/index.jpg}
%     \caption{Created index in FAISS}
% \end{figure}

\section{Semantic Search and Thesis Retrieval System}

The semantic search and thesis retrieval system addresses second specific objective by using the RAG pipeline with Google Gemini 2.5-flash. In this way, the system is evolving from a static repository of documents to a dynamic, user intent based discovery of information, accurately retrieving relevant academic content.

\subsection{Query Encoding and Retrieval}
Queries were encoded with the same model as used for indexing for consistency. The retriever powered by FAISS returned the top-$K$ chunks to balance precision and recall.

For example, when users asked, “What research has been done on machine learning applications in healthcare?”, as shown in \ref{fig:sample_query_output} the system retrieved abstracts/summary and key sections.  Notably, setting $K=50$ produced a good balance of focused context and cross-thesis coverage.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/virgo_sample_query.jpg}
    \caption{Screenshot of Query and Retrieved Output}
    \label{fig:sample_query_output}
\end{figure}

\ref{fig:sample_query_output} shows a sample user query about existing research on machine learning applications in healthcare and the retrieved thesis key sections and summary. The system effectively find one thesis related to the query, which demonstrates its capability to locate relevant chunk content from the FAISS vector database.

Moreover, the output also includes the created citations that shows the author's last name and the year of publication in the CSPC. Notably, this retrieved data are all chunks that was get from the top-K retrieval process.

\subsection{Augmented Input Generation}
Retrieved chunks were concatenated with the user query into a structured context that included lightweight citation markers and the URL of the source document. This construction supported grounded, traceable responses and minimized the risk of hallucinations. Prompt templates instructed the model to answer only based on the provided context with safeguards to maintain quality input as shown in \ref{fig:sample_query_output}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/virgo_bomb_query.jpg}
    \caption{Screenshot of Sensitive Query and Output Generated}
    \label{fig:sample_bomb_query}
\end{figure}

\ref{fig:sample_bomb_query} presents a user query that is classified to be filtered out along with the response generated through the RAG chatbot. The system clearly identified that the user query is not allowed based on the system prompt for safety features encoded within the implementation. This supports the efficiency of the chatbot in the handling of sensitive questions through warnings rather than generating harmful or offensive content.

\subsection{Response Generation with Gemini 2.5-flash }

The Gemini 2.5-flash model provided responses based on the retrieved context. Temperature = 0 was used as input to ensure deterministic outputs suitable for scholarly use case. Clean text was generated by parsing the output. RAG substantially reduces hallucinations, but inaccuracies sometimes happened if not enough context was available, therefore, users are encouraged to double-check critical conclusions.

\subsection{Interface and Usage Observations}

The Flask-based web interface supported conversational exploration with session-based history and safety filters for disallowed queries.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/adalv4.jpg}
    \caption{User Interface}
    \label{fig:flask_interface}
\end{figure}

Generated responses appeared as Markdown with citations and structured text. When queries violated safety parameters, clear warnings were shown. Deterministic settings improved consistency and user trust.

\section{Model Evaluation}
This section reveals the evaluation result of the RAG chatbot using the four metrics from RAGAS including Answer Relevancy, Context Precision, Context Recall, and Faithfulness. Each metrics captures the system accuracy, coverage, and grounding of responses. Another method applied for evaluation is the user-centered evaluation specifically using a 5-point Likert scale questionnaire.

\subsection{RAG System Evaluation Results}
The evaluation result of the RAG system using the four core metrics from the RAGAS framework summarized in \ref{tab:rag_metrics} highlights its effectiveness in retrieving and generating accurate, relevant, and well-grounded answers based on the indexed thesis documents from the CSPC Library.

\begin{table}[H]
    \centering
    \caption{RAG System Evaluation Metrics using RAGAS Framework}
    \label{tab:rag_metrics}
    \begin{tabular}{ll} 
        \hline
        \textbf{Metric}     & \textbf{Average Score} \\
        \hline
        Faithfulness        & 0.9179 \\
        Context Precision   & 0.9167 \\
        Context Recall      & 0.8711 \\
        Answer Relevancy    & 0.8625 \\
        \hline
    \end{tabular}
\end{table}

\ref{tab:rag_metrics} shows how the high result of the RAG system performance in terms of Faithfulness, Context Precision (0.9167), Context Recall (0.8711), and Answer Relevancy (0.8625), where all these result point to a reliable and well-grounded responses.

These results show that the system retrieves appropriate and focused evidence, covers a wide range of relevant thesis content, and the generated answers correspond to user intent. This is in line with previous work on RAG-based academic retrieval systems: first, the key to trustworthy outputs rests on grounding and precision \cite{lewis2020retrieval}.

The results indicated that the system could be used as a useful academic support tool, which would improve the literature search to find theses for students and researchers. For some reason, the current evaluation result is not considered the absolute reflection of the system performance, which might affect generalizability. Future research should examine performance on more diverse or ambiguous queries and compare results with alternative retrieval models to further validate and improve the system.

\subsubsection{\textbf{Visualization of RAG System Evaluation Results}}
The figures below illustrate the evaluation metrics results of the RAG system using various visualization techniques, including bar charts, heatmaps, and radar charts.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/RAG_bar_chart_result.png}
    \caption{Bar Chart of RAG System Evaluation Result}
    \label{fig:rag_bar_chart}
\end{figure}

As shown in the bar graph is the evaluation result of the RAG system, which performs consistently well on all four core metrics: Faithfulness, Context Precision, Context Recall, and Answer Relevancy. Among these, Faithfulness reaches 0.918 and Context Precision reaches 0.917, which are the highest values, confirming that the system consistently provides responses with a proper grounding in accurate and relevant information from CSPC thesis documents. The very high precision in this regard would suggest that the chatbot reduces hallucinations and retrieves the most relevant segments consistently, which is an important consideration in academic work, since the accuracy and relevance of facts are highly important.

Slightly lower scores in Context Recall and Answer Relevancy, though solid, would suggest that the system captures a substantial portion of relevant information and generally aligns with user queries, even if sometimes gaps exist in comprehensiveness or directness.These findings confirm previous studies on RAG frameworks in academic retrieval systems, which highlight that responses should be based on source material with a high degree of precision to enable trustworthy outputs \cite{lewis2020retrieval}.The strong performance observed for the Faithfulness and Context Precision metrics provides an empirical confirmation of the theoretical proposition that a well-optimized RAG pipeline can considerably reduce hallucinations and allow for more reliable generated answers.

Performing semantic search and vector-based retrieval allows the system to go beyond the conventional keyword-based method and provides more accurate and contextually relevant information. Integrating RAG with large language models such as Gemini 2.5-flash demonstrates practical value in combining retrieval and generation into tools in support of academic work.

Despite of having positive and encouraging results, there are still a number of limitations: the relatively lower scores for Context Recall and Answer Relevancy indicate that the system occasionally might miss out on some information or give responses that could be more complete. These addressed potential point can lead to further enhancements that can be made to ensure all relevant information is consistently retrieved and that responses fully address user intent. Future efforts should seek to optimize chunking strategies, increase the diversity of ingested documents, and investigate adaptive retrieval parameters to further improve recall and relevancy. Moreover, comparisons with other retrieval models or more diverse sets of queries could go deeper in generalizability and robustness of the system. Solving these limitations will further enhance the RAG chatbot's effectiveness as an academic search assistant and facilitate more complete and satisfactory literature discovery for students and researchers.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/RAG_heatmap_chart_result.png}
    \caption{Heatmap of RAG System Evaluation Result}
    \label{fig:rag_heatmap}
\end{figure}

The heatmap of the evaluation results of the RAG system shows that it consistently produces robust outputs for most of the questions, while its scores range from 0.75 to 1.00. Dark green cells reflect high-quality outputs, while mid-range yellow tones and a single red cell highlight low Context Precision for Question 3, which indicates an area where context selection could be further improved. Overall, the system is strong regarding answer alignment, most questions attained scores above 0.90 for Answer Relevancy, whereas Questions 3 and 4 had slightly lower relevancy, which may point to some gaps in information retrieved.

These findings confirm the system’s robustness in terms of grounding responses and selecting relevant context, it supported by the previous literature that emphasizes precision and grounding within academic retrieval systems. However, given the found limitations particularly in the low Context Precision for Question 3 and reduced relevancy for queries in certain targeted cases where refinements will be necessary, especially for questions which could be seen as more ambiguous or complex. In addition, future research should focus on optimizing the context selection strategy and diversifying the representation of ingested documents to boost recall and relevance. This would ensure the system retains its robustness and effectiveness for an increasingly wide range of academic queries.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/RAG_radar_chart_result.png}
    \caption{Radar Chart of RAG System Evaluation Result}
    \label{fig:rag_radar_chart}
\end{figure}

This radar charts shows that the performance of the RAG system is consistently high and balanced for the four evaluation metrics: Faithfulness, Context Precision, Context Recall, and Answer Relevancy. The chart is close to being symmetric due to having no metric performs worse than the other, but chatbot Faithfulness and Context Precision show to be in high performance. It means that the system tends to provide responses based on the retrieved documents and selects context highly relevant for the user's query, thus avoiding hallucinations and maintaining strong alignment with source documents.

Still, the Context Recall and the Answer Relevancy metrics showed in the Radar chart has very good results, while the visualization presents a number of further steps that can be taken to continue on improving the system's ability to retrieve all relevant information and provide answers that would fully meet user expectation and it would increase user interaction to use the chatbot more. All these further steps for improvement may provide even more comprehensive and satisfying answers for the next system iterations.

These overall visualization results of evaluation metrics confirm the RAG system's capability as a dependable academic search assistant, while also guiding future enhancements to further elevate its performance.

\subsection{User-Centered Evaluation Results}
The user-centered evaluation results of the RAG chatbot using a 5-point Likert scale survey are presented in the subsequent sections. This evaluation method allows users to provide feedback on the chatbot's response quality, performance, effectiveness, and usability from their perspective.

\subsubsection{\textbf{User Agreement on Chatbot Response Quality and Performance}}
\ref{tab:user_agreement_quality} shows the results of the user-centered evaluation of the CSPC Library RAG chatbot using 5-point likert scale survey questions that allows the respondents to evaluate and choose the level of agreement with the chatbot’s response quality and performance.

\begin{table}[H]
    \centering
    \caption{User Agreement: Chatbot Response Quality and Performance}
    \label{tab:user_agreement_quality}
    \footnotesize
    \begin{tabular}{p{7cm} c c}
        \hline
        \multicolumn{1}{c}{\textbf{Criteria}} & \textbf{Weighted Mean} & \textbf{Verbal Interpretation} \\
        \hline
        The questions are answered well by the chatbot. & 4.4 & Strongly Agree \\
        \hline
        The answers are relevant to the question. & 4.6 & Strongly Agree \\
        \hline
        Chatbot’s responses are clear and understandable. & 4.5 & Strongly Agree \\
        \hline
        The chatbot’s responses help answer your questions. & 4.5 & Strongly Agree \\
        \hline
        The chatbot provided enough information. & 4.4 & Strongly Agree \\
        \hline
        The chatbot has a quick response time. & 4.5 & Agree \\
        \hline
        \textbf{Overall Weighted Mean} & \textbf{4.5} & \textbf{Strongly Agree} \\
        \hline
    \end{tabular}
\end{table}

The evaluation result of the RAG chatbot using the 5 Point Likert Scale which is the method for user-centered evaluation showed a positive indication from users. We found that the chatbot is good at question \& answers and users strongly agreed with a weighted mean of 4.4, indicating that chatbot's answering user questions meets the expectations of the users in getting right answers. The chatbot is also good at interpreting user intent and provide relevant answers based on the questions, it's proven by the users which they strongly agreed with a weighted mean of 4.6. Furthermore, it is also evident that the chatbot gives clear and easy to understand answers. This means that the chatbot not only gives correct responses but also explains it to the users that can easily be understood and this statement is supported with the users strong agreement with the weighted mean of 4.5. It is also evident that the chatbot helped users find the answers they really need. They strongly agree with a weighted mean of 4.5 that the chatbot's replies were helpful and relevant to the user's questions. Moreover, the users are satisfied with the chatbot's provided information which means it gave complete and very useful answers during their interaction where the users strongly agree in that sense with weighted mean of 4.4. Lastly, the users strongly agree (weighted mean: 4.5) that the chatbot is quick to reply and responded without any delay. This means that the system provides the answers fast, helping users get information they need for research on time. Overall, the respondent users of the chatbot gain an average weighted mean of 4.5 (Strongly Agree). This only mean that users found the chatbot's responses correct, relevant, clear, mostly complete, and that the chatbot responded quickly helping users research fast. This survey findings from students and faculty responses proves that the chatbot works well for its main purpose of helping users find information and understand answers in academic context. For the future improvement of the RAG-chatbot, it's important to make responses more complete and slightly faster to raise overall satisfaction even more.  And, according to \citeauthor{folstad2021future} \citeyear{folstad2021future}, user-centered evaluation has been key within several disciplines at the roots of current chatbot research, particularly in understanding users' needs, motivations, and experiences with chatbot interactions. Thus, it is advisable to utilize this method to assess system effectiveness and user satisfaction before deployment to ensure the RAG chatbot meets actual user expectations and provides satisfactory support for thesis retrieval tasks in the CSPC Library context.

\subsubsection{\textbf{User Feedback on RAG chatbot’s Effectiveness and Usability}}
\ref{tab:user_feedback_table} presents the user-centered evaluation results of the RAG chatbot using a 5-point Likert scale. The table shows weighted means for user satisfaction, likelihood of using the chatbot again, ease of reading and understanding the chatbot’s output, and confidence in the chatbot’s information, allowing readers to gauge overall user perception and intent to use the system in the future.

\begin{table}[H] %%%%% Table 6 page 55 %%%%%%
    \centering
    \caption{User Feedback on RAG chatbot’s Effectiveness and Usability}
    \label{tab:user_feedback_table}
    \footnotesize
    \begin{tabular}{m{5cm} c c}
        \hline
        \multicolumn{1}{c}{\textbf{Criteria}} & \textbf{Weighted Mean} & \textbf{Verbal Interpretation} \\
        \hline
        Satisfaction with answers & 4.3 & Satisfied \\
        \hline
        Likelihood of using the chatbot again & 4.3 & Very Likely \\
        \hline
        Ease of understanding the chatbot’s output & 4.6 & Very Easy \\
        \hline
        Confidence in the chatbot’s information & 4.1  Confident \\
        \hline
        \textbf{Overall Weighted Mean} & \textbf{4.3} & \textbf{Strongly Agree} \\
        \hline
    \end{tabular}
\end{table}

The RAG chatbot result whether it is effective and useful are made possible through user responses, summarized in \ref{tab:user_feedback_table}, shows that users are very satisfied with answers with a weighted mean of 4.3, we also found that users would likely use the chatbot again in future use case for their research with the weighted mean result of 4.3, the chatbot was also proven to have outputs that are easy to understand by users which they agree with the weighted mean of 4.6. Furthermore, the users are moderately confident in information provided by the RAG-chatbot with the weighted mean result of 4.1. Overall, the respondents of the chatbot's effectiveness and usefulness gain an average weighted mean of 4.3 which means that the chatbot gives what the responses user needs, it also provides clear responses, easy to navigate interface, and a good tool for academic research assistance and retrieval. The studies of \citeauthor{kaushal2022role} and \citeauthor{okonkwo2021chatbots} supported our findings, highlighting the importance of clarity and usefulness in increasing user satisfaction with chatbots or applications. Further, the researches by \citeauthor{choudhury2023investigating} and \citeauthor{zhang2024ai} highlight the importance of trust and correct facts to keep users using AI chatbots with confidence in academic purpose. The RAG chatbot has the enormous potential to support academic users, which enhances future use as well as the research process. Nevertheless, the moderate confidence level indicates that the user should look forward to improvements in factual accuracy. Future research should be directed towards improving the validation of information by the chatbot as well as increasing transparency to enhance user confidence to support academic users consistently and dependably.

%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}