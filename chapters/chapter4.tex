\chapter{Results and Discussion}
\begin{refsection}

This chapter discusses the results and evaluation of the RAG chatbot developed for efficient literature search and thesis retrieval at the CSPC Library.

\section{Document Ingestion and Retrieval Module}

This implementation addressed the first specific objective of the study by transforming the library's static collection of thesis PDFs into a dynamic, searchable knowledge base. 

\subsection{Dataset and Preparation}
The study corpus comprised all available undergraduate thesis PDFs from multiple CSPC departments (290+ documents). The dataset was prepared via structured text extraction and token-based chunking aligned with thesis sections (Abstract; Chapters 1-5).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/dataset_sample.jpg}
    \caption{CSPC Thesis PDF Sample}
    \label{fig:dataset_sample}
\end{figure}

% Upon agreement on project scope and data handling, library personnel granted the researchers to gain access to the digital copies of undergraduate thesis papers. This composes of theses from different departments.

Upon agreement on project scope and data handling, library personnel granted the researchers to gain access to the available digital copies of undergraduate thesis papers. 

\subsection{Data Preprocessing}
Texts were extracted page by page and enriched with metadata (source, page) to preserve academic provenance. Token-based chunking produced coherent segments sized to the LLM context window and guided by thesis structure, improving retrieval fidelity and citation transparency.

\begin{table}[H]
    \centering
    \caption{Chunk Analysis \& Statistics}
    \label{tab:chunk_analysis_stats}
    \begin{tabular}{ll} 
        \hline
        \textbf{Metric}            & \textbf{Value} \\
        \hline
        Total Chunks               & 38,127 \\
        Total Tokens               & 11,849,783 \\
        Avg Characters/Chunk       & 1323 \\
        Avg Words/Chunk            & 180 \\
        Minimum Tokens per chunk   & 124 \\
        Maximum Tokens  per chunk  & 1200 \\
        Median Tokens per chunk    & 335 \\
        \hline
    \end{tabular}
\end{table}

The overall results of the chunk analysis as shown in \ref{tab:chunk_analysis_stats} indicate that the implemented chunking strategy is effective and well-structured for document preprocessing. The analysis produced a total of 38,127 chunks with 11,849,783 tokens, where each chunk contains an average of 1,323 characters or approximately 180 words, and a median of 335 tokens per chunk. These results show that the generated chunks fall within an appropriate size range to preserve semantic context while remaining suitable for vector embedding and retrieval.

In terms of chunk size control, the results demonstrate that the system successfully enforced defined boundaries. The minimum chunk size was 124 tokens, which prevented the creation of fragmented or low-information chunks that could negatively impact embedding quality. Meanwhile, the maximum chunk size was limited to 1,200 tokens, ensuring that excessively large chunks that could dilute semantic relevance were avoided.

\subsection{Indexing and Vector Database Construction}
The indexing phase transformed the preprocessed text chunks into a searchable knowledge base optimized for semantic retrieval within the RAG pipeline. This critical stage bridged the gap between raw textual content and the intelligent query-response capabilities that would define the chatbot's effectiveness in academic literature discovery.

Embeddings were generated primarily with sentence-transformers/all-MiniLM-L6-v2 (HuggingFace), chosen for its efficiency and strong semantic performance. FAISS stored vectors alongside page metadata to preserve traceability. This enabled natural language queries to retrieve semantically relevant thesis segments beyond exact keyword matching.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures/index.jpg}
%     \caption{Created index in FAISS}
% \end{figure}

\section{Semantic Search and Thesis Retrieval System}

The semantic search and thesis retrieval system addresses the second specific objective by leveraging the RAG pipeline and Google Gemini 2.5-flash. This implementation transitions the system from static document storage to dynamic, intent-driven information discovery, enabling precise retrieval of relevant academic content.

\subsection{Query Encoding and Retrieval}
Queries were embedded using the same model as indexing to ensure consistency. The FAISS-backed retriever returned the top-$K$ chunks, balancing precision and recall.

For example, when users asked, “What research has been done on machine learning applications in healthcare?”, as shown in \ref{fig:sample_query_output} the system retrieved abstracts/summary and key sections.  Notably, setting $K=50$ produced a good balance of focused context and cross-thesis coverage.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/virgo_sample_query.jpg}
    \caption{Screenshot of Query and Retrieved Output}
    \label{fig:sample_query_output}
\end{figure}

\ref{fig:sample_query_output} shows a sample user query about existing research on machine learning applications in healthcare and the retrieved thesis key sections and summary. The system effectively find one thesis related to the query, which demonstrates its capability to locate relevant chunk content from the FAISS vector database.

Furthermore, the output includes the generated citations that show the author's last name and the year of publication, as well as the thesis URL, which can be used to browse and retrieve the entire pdf thesis copy from Google Drive.

\subsection{Augmented Input Generation}
Retrieved chunks were concatenated with the user query into a structured context with lightweight citation markers and including the url of the source document. This supported grounded, traceable answers and reduced hallucination risk. Prompt templates guided the model to answer strictly from provided context, with guardrail to maintain input quality as shown in \ref{fig:sample_bomb_query}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/virgo_bomb_query.jpg}
    \caption{Screenshot of Sensitive Query and Output Generated}
    \label{fig:sample_bomb_query}
\end{figure}

\ref{fig:sample_bomb_query} shows a sample user query that is sensitive in nature and the output generated by the RAG chatbot. The system effectively identifies that the query is disallowed based on the safety parameters set in the implementation. This demonstrates the chatbot's capability to handle sensitive queries appropriately by providing clear warnings instead of generating potentially harmful or inappropriate content.

\subsection{Response Generation with Gemini 2.5-flash}

The Gemini 2.5-flash model generated response grounded in retrieved context. The system was configured with temperature=0 (K=0) to favor greedy selection. This ensured a deterministic outputs that prioritized accuracy above creativity. Besides, this have greatly reduces hallucinations from the testing.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.93\textwidth]{figures/almira_query.jpg}
        \subcaption{User 1 Query and Response}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.93\textwidth]{figures/franco_query.jpg}
        \subcaption{User 2 Query and Response}
    \end{minipage}
    \caption{Comparison of outputs from two user with the same query}
    \label{fig:comparison_two_users}
\end{figure}

By setting Temperature=0, the generation part produced the same output for all exact same prompts. As shown in \ref{fig:comparison_two_users}, a comparison of outputs from two different users who prompted the same query and they retrieved same output. Both users asked, "Recommend 1 thesis related to sweet foods for business. I only need title, short summary, url and cite" and received deterministic output. By setting this parameter, it helps in mitigating non-factual output as randomness of used words are low. However, 
while it significantly reduced hallucinations, some inaccuracies were observed when context and prompts was insufficient. And so, users were advised to validate the output and check the whole pdf.


% \subsection{Interface and Usage Observations}

% The Flask-based web interface supported conversational exploration with session-based history and safety filters for disallowed queries.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures/adalv4.jpg}
%     \caption{User Interface}
%     \label{fig:flask_interface}
% \end{figure}

% Generated responses appeared as Markdown with citations and structured text. When queries violated safety parameters, clear warnings were shown. Deterministic settings improved consistency and user trust.

\section{Model Evaluation}
The third objective focuses on evaluating the performance of the RAG chatbot using RAGAS and user satisfaction metrics. This section presents the results from both the RAGAS  framework and user-centered evaluation from a 5-point Likert scale questionnaire.

\subsection{RAG System Evaluation Results}
The first evaluation of the RAG system was assessed using the RAGAS framework, with its metrics including Faithfulness, Context Precision, Context Recall, and Answer Relevancy, as shown in \ref{tab:rag_metrics}.

\begin{table}[H]
    \centering
    \caption{RAG System Evaluation Metrics using RAGAS Framework}
    \label{tab:rag_metrics}
    \begin{tabular}{ll} 
        \hline
        \textbf{Metric}     & \textbf{Average Score} \\
        \hline
        Faithfulness        & 0.9179 \\
        Context Precision   & 0.9167 \\
        Context Recall      & 0.8711 \\
        Answer Relevancy    & 0.8625 \\
        \hline
    \end{tabular}
\end{table}

\ref{tab:rag_metrics} shows a promising average score result from the RAGAS evaluation metrics. The faithfulness achieved an average score of 0.9179, which indicates that the RAG system was consistent in generating responses directly supported by the information present in the retrieved context from the thesis chunks. Context precision of 0.9167 confirms that the retrieved chunks were ranked as the most highly relevant chunk for the user’s query. The context recall also had a considerably high average score of 0.8711, indicating that the RAG system successfully retrieved most of the relevant information necessary to answer the query. And lastly, the answer relevancy which had an average score of 0.8625, indicates that the answer generated by the RAG system was highly relevant to the specific query asked by the user.

Overall, these results show that the system retrieves appropriate and focused evidence, covers a wide range of relevant thesis content, and the generated answers correspond to user intent. This is in line with previous work on RAG-based academic retrieval systems: first, the key  to trustworthy outputs rests on grounding and precision \cite{lewis2020retrieval}.

% Overall, the results indicate that the system could be used as a useful research tool, which could revolutionize thesis retrieval in the CSPC library. Though, for some reason, the current evaluation result is not considered the absolute reflection of the system performance, which might affect generalizability. Future work will further compare the performance on a broader class of queries, including diverse or ambiguous ones, against alternative retrieval models in order to validate and enhance this RAG system.

\subsubsection{\textbf{Visualization of RAG System Evaluation Results}}
The figures below illustrate the various metrics on evaluating the RAG system, using a variety of visualization techniques such as bar charts, heatmaps, and radar charts.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/RAG_bar_chart_result.png}
    \caption{Bar Chart of RAG System Evaluation Result}
    \label{fig:rag_bar_chart}
\end{figure}

As shown in bar graph in \ref{fig:rag_bar_chart} is the evaluation result of the RAG system, which performs consistently well on all four core metrics: Faithfulness, Context Precision, Context Recall, and Answer Relevancy. Among these, Faithfulness reaches 0.918 and Context Precision reaches 0.917, which are the highest values, confirming that the system consistently provides responses with a proper grounding in accurate and relevant information from CSPC thesis documents.The very high precision in this regard would suggest that the chatbot reduces hallucinations and retrieves the most relevant segments consistently, which is an important consideration in academic work, since the accuracy and relevance of facts are highly important.

Slightly lower performance in Context Recall and Answer Relevancy, while good result, it indicates that the system nevertheless captures a large part of the relevant information and generally aligns to user queries, though at times with some gaps in completeness or directness. These results are of course in line with previous work focused on RAG frameworks in academic retrieval systems and their calls to responses by source evidence with high precision to facilitate dependable output \cite{lewis2020retrieval}.

The strong performance observed for the Faithfulness and Context Precision metrics provides an empirical confirmation of the theoretical proposition that a well-optimized RAG pipeline can considerably reduce hallucinations and allow for more reliable generated answers. Performing semantic search and vector-based retrieval allows the system to go beyond the conventional keyword-based method and provides more accurate and contextually relevant information. Integrating RAG with large language models such as Gemini 2.5-flash demonstrates practical value in combining retrieval and generation into tools in support of academic work.

Despite of having positive and encouraging results, there are still a number of limitations: the relatively lower scores for Context Recall and Answer Relevancy indicate that the system occasionally might miss out on some information or give responses that could be more complete. These addressed potential point can lead to further enhancements that can be made to ensure all relevant information is consistently retrieved and that responses fully address user intent. 

Future efforts should seek to optimize chunking strategies, increase the diversity of ingested documents, and investigate adaptive retrieval parameters to further improve recall and relevancy. Moreover, comparisons with other retrieval models or more diverse sets of queries could go deeper in generalizability and robustness of the system. Solving these limitations will further enhance the RAG chatbot's effectiveness in thesis retrieval and facilitate more complete and satisfactory literature discovery for students and researchers.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/RAG_heatmap_chart_result.png}
    \caption{Heatmap of RAG System Evaluation Result}
    \label{fig:rag_heatmap}
\end{figure}

The heatmap of the evaluation results of the RAG system as shown in \ref{fig:rag_heatmap} has consistently produces robust outputs for most of the questions, while its scores range from 0.75 to 1.00. Dark green cells reflect high-quality outputs, while mid-range yellow tones and a single red cell highlight low Context Precision for Question 3, which indicates an area where context selection could be further improved. Overall, the system is strong regarding answer alignment, most questions attained scores above 0.90 for Answer Relevancy, whereas Questions 3 and 4 had slightly lower relevancy, which may point to some gaps in information retrieved.

These findings confirm the system’s robustness in terms of grounding responses and selecting relevant context, it supported by the previous literature that emphasizes precision and grounding within academic retrieval systems. However, given the found limitations particularly in the low Context Precision for Question 3 and reduced relevancy for queries in certain targeted cases where refinements will be necessary, especially for questions which could be seen as more ambiguous or complex. In addition, future research should focus on optimizing the context selection strategy and diversifying the representation of ingested documents to boost recall and relevance. This would ensure the system retains its robustness and effectiveness for an increasingly wide range of academic queries.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/RAG_radar_chart_result.png}
    \caption{Radar Chart of RAG System Evaluation Result}
    \label{fig:rag_radar_chart}
\end{figure}

\ref{fig:rag_radar_chart} shows a radar chart illustrating the performance of the RAG system is high and well balanced for the four different evaluation metrics: Faithfulness, Context Precision, Context Recall, and Answer Relevancy. The chart is close to being symmetric due to having no metric performs worse than the other, but chatbot Faithfulness and Context Precision show to be in high performance. It means that the system tends to provide responses based on the retrieved documents and selects context highly relevant for the user's query, thus avoiding hallucinations and maintaining strong alignment with source documents.Still, the Context Recall and the Answer Relevancy metrics showed in the Radar chart has very good results, while the visualization presents a number of further steps that can be taken to continue on improving the system's ability to retrieve all relevant information and provide answers that would fully meet user expectation and it would increase user interaction to use the chatbot more.All these further steps for improvement may provide even more comprehensive and satisfying answers for the next system iterations.

These overall visualization results of evaluation metrics confirm the RAG system's capability as a dependable academic search assistant, while also guiding future enhancements to further elevate its performance.

\subsection{User-Centered Evaluation Results}
The user-centered evaluation results of the RAG chatbot using a 5-point Likert scale survey are presented in the subsequent sections. The results of user-centered evaluations of the RAG chatbot, obtained through a 5-point Likert-scale survey, are presented in the following sections. This evaluation method allows users to give their opinion about response quality, performance, effectiveness, and usability of the chatbot.

\subsubsection{\textbf{User Agreement on Chatbot Response Quality and Performance}}
\ref{tab:user_agreement_quality} shows the results of the user-centered evaluation of the CSPC Library RAG chatbot through a 5-point Likert-scale survey that allows the respondents to judge and express the level of their agreement regarding the chatbot’s response quality and performance.

\begin{table}[H]
    \centering
    \caption{User Agreement: Chatbot Response Quality and Performance}
    \label{tab:user_agreement_quality}
    \footnotesize
    \begin{tabular}{p{7cm} c c}
        \hline
        \multicolumn{1}{c}{\textbf{Criteria}} & \textbf{Weighted Mean} & \textbf{Verbal Interpretation} \\
        \hline
        The questions are answered well by the chatbot. & 4.4 & Strongly Agree \\
        \hline
        The answers are relevant to the question. & 4.6 & Strongly Agree \\
        \hline
        Chatbot’s responses are clear and understandable. & 4.5 & Strongly Agree \\
        \hline
        The chatbot’s responses help answer your questions. & 4.5 & Strongly Agree \\
        \hline
        The chatbot provided enough information. & 4.4 & Strongly Agree \\
        \hline
        The chatbot has a quick response time. & 4.5 & Agree \\
        \hline
        \textbf{Overall Weighted Mean} & \textbf{4.5} & \textbf{Strongly Agree} \\
        \hline
    \end{tabular}
\end{table}

The evaluation result of the RAG chatbot using the 5 Point Likert Scale which is the method for user-centered evaluation showed a positive indication from users. We found that the chatbot is good at question \& answers and users strongly agreed with a weighted mean of 4.4, indicating that chatbot's answering user questions meets the expectations of the users in getting right answers. The chatbot is also good at interpreting user intent and provide relevant answers based on the questions, it's proven by the users which they strongly agreed with a weighted mean of 4.6. Furthermore, it is also evident that the chatbot gives clear and easy to understand answers.This means that the chatbot not only gives correct responses but also explains it to the users that can easily be understood and this statement is supported with the users strong agreement with the weighted mean of 4.5. It is also evident that the chatbot helped users find the answers they really need. They strongly agree with a weighted mean of 4.5 that the chatbot's replies were helpful and relevant to the user's questions. Moreover, the users are satisfied with the chatbot's provided information which means it gave complete and very useful answers during their interaction where the users strongly agree in that sense with weighted mean of 4.4. Lastly, the users strongly agree (weighted mean: 4.5) that the chatbot is quick to reply and responded without any delay. This means that the system provides the answers fast, helping users get information they need for research on time. Overall, the respondent users of the chatbot gain an average weighted mean of 4.5 (Strongly Agree). This only mean that users found the chatbot's responses correct, relevant, clear, mostly complete, and that the chatbot responded quickly helping users research fast. This survey therefore shows that responses from students and faculty confirm the effectiveness of the chatbot in its primary function, which is to assists users in finding information to clarify answers within an academic context. For future improvements of the RAG-chatbot, responses should be more complete and slightly faster to further improve overall user satisfaction. Studies of \citeauthor{folstad2021future} [\citeyear{folstad2021future}] they stated that, user-centered evaluation has important role in several disciplines that highlighted modern research on chatbots, especially in understanding users' needs, motivations, and experiences related to the interactions with chatbots. Therefore, an evaluation approach is recommended before deployment of the system, to consider aspects like the effectiveness of the system and user satisfaction. Therefore, this evaluative approach is recommended before system deployment to investigate aspects of system effectiveness and user satisfaction. The result of this user evaluation ensure that the RAG chatbot meets the expectations of real users and provides effective support in tasks related to the retrieval of theses within the CSPC Library context.

\subsubsection{\textbf{User Feedback on RAG chatbot’s Effectiveness and Usability}}

\ref{tab:user_feedback_table} presents the results of the user-centered evaluation of the RAG chatbot, using a five-point Likert scale. The result in weighted means for user satisfaction, the likelihood of use in the future, ease of reading and understanding the chatbot's output, and confidence in the information given by the chatbot enable the reader to understand overall user perception and intended future use of the system.

\begin{table}[H] %%%%% Table 6 page 55 %%%%%%
    \centering
    \caption{User Feedback on RAG chatbot’s Effectiveness and Usability}
    \label{tab:user_feedback_table}
    \footnotesize
    \begin{tabular}{m{5cm} c c}
        \hline
        \multicolumn{1}{c}{\textbf{Criteria}} & \textbf{Weighted Mean} & \textbf{Verbal Interpretation} \\
        \hline
        Satisfaction with answers & 4.3 & Satisfied \\
        \hline
        Likelihood of using the chatbot again & 4.3 & Very Likely \\
        \hline
        Ease of understanding the chatbot’s output & 4.6 & Very Easy \\
        \hline
        Confidence in the chatbot’s information & 4.1 & Confident \\
        \hline
        \textbf{Overall Weighted Mean} & \textbf{4.3} & \textbf{Strongly Agree} \\
        \hline
    \end{tabular}
\end{table}

The RAG chatbot result whether it is effective and useful are made possible through user responses, summarized in \ref{tab:user_feedback_table}, shows that users are very satisfied with answers with a weighted mean of 4.3, we also found that users would likely use the chatbot again in future use case for their research with the weighted mean result of 4.3, the chatbot was also proven to have outputs that are easy to understand by users which they agree with the weighted mean of 4.6. Furthermore, the users are moderately confident in information provided by the RAG-chatbot with the weighted mean result of 4.1. Overall, the respondents of the chatbot's effectiveness and usefulness gain an average weighted mean of 4.3 which means that the chatbot gives what the responses user needs, it also provides clear responses, easy to navigate interface, and a good tool for academic research assistance and retrieval. Based on the studies of \citeauthor{kaushal2022role} and \citeauthor{okonkwo2021chatbots} supported our findings, highlighting the importance of clarity and usefulness in increasing user satisfaction with chatbots or applications. Further, the researches by \citeauthor{choudhury2023investigating} and \citeauthor{zhang2024ai} highlight the importance of trust and correct facts to keep users using AI chatbots with confidence in academic purpose. The RAG chatbot has the enormous potential to support academic users, which enhances future use as well as the research process. Nevertheless, the moderate confidence level indicates that the user should look forward to improvements in factual accuracy. Future research should be directed towards improving the validation of information by the chatbot as well as increasing transparency to enhance user confidence to support academic users consistently and dependably.

%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}