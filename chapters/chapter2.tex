\chapter{Related Literature and Studies}
\begin{refsection}

This chapter presents an analysis of relevant literature and existing systems associated with the study. It includes a summary of related works, a synthesis of the state-of-the-art technologies and methodologies, and identifies the research gaps addressed by the current study.


\section{Review of Related Literature and Studies}

To develop a deeper understanding of the research topic, a comprehensive review of books, scholarly articles, journals, and previous thesis projects was conducted. The findings are organized thematically to align with the key areas of the study.


\subsection{Large Language Models}

\hspace{1cm}Large Language Models (LLMs) have significantly enhanced information retrieval (IR) capabilities, especially within academic environments where timely and accurate access to scholarly information is essential. The integration of LLMs such as ChatGPT and other transformer-based architectures, has led to major improvements in natural language processing (NLP), enabling more effective question-answering, summarization, document classification, and content generation \cite{yalamanchili2024quality} \cite{yang2023large}. Studies by \citeauthor{khraisha2024can} \citeyear{khraisha2024can} and \citeauthor{gartlehner2023data} \citeyear{gartlehner2023data}demonstrate that LLMs can streamline academic workflows by automating processes such as systematic reviews, data extraction, and document screening, showcasing the models' utility in improving the efficiency and scalability of research tasks \cite{khraisha2024can}  \cite{gartlehner2023data}.

\hspace{0.4cm}Despite their advantages, LLMs present notable limitations in domain-specific tasks that require expert-level knowledge. These models rely heavily on pre-trained knowledge and lack direct access to real-time, domain-specific datasets unless explicitly fine-tuned. Omar et al. note that LLMs, including ChatGPT, often fall short when handling complex academic queries due to their generalized training data, reducing their effectiveness in specialized contexts such as law, medicine, and academia \cite{khraisha2024can}. Lucas et al. further emphasized that the static nature of LLMs hinders their ability to adapt to the continuously evolving corpus of academic literature, limiting their utility in real-time or up-to-date research scenarios \cite{gartlehner2023data}.

\hspace{0.4cm}Security and privacy concerns also arise in the deployment of LLMs, particularly when used via cloud-based services or APIs. Achar and Xian et al. point out that the use of Machine Learning-as-a-Service (MLaaS) introduces risks of data breaches, unauthorized access, and loss of data sovereignty \cite{omar2024applications} \cite{lucas2024systematic}. In contrast, \citeauthor{thapa2022splitfed} \citeyear{thapa2022splitfed} advocate for local deployment of LLMs on private infrastructures, citing improvements in privacy and security. Their work, titled "Splitfed: When Federated Learning Meets Split Learning," demonstrates that local deployment reduces the attack surface and enhances user control over sensitive academic and research data \cite{achar2018data}

\hspace{0.4cm}To address the limitations of LLMs, recent studies have explored hybrid approaches such as Retrieval-Augmented Generation (RAG). This framework improves LLM performance in domain-specific applications by dynamically retrieving relevant external documents during inference. Evaluations of such systems employ various metrics, including Context Precision, Context Recall, and Faithfulness. For instance, \citeauthor{yalamanchili2024quality} \citeyear{yalamanchili2024quality} conducted experiments measuring context precision (the proportion of relevant retrieved passages used in responses) and faithfulness (factual alignment between model-generated responses and retrieved documents). Their study reported an improvement of over 30 percent in context precision and 25 percent in faithfulness when RAG was integrated into academic query handling, compared to standard LLM responses \cite{yalamanchili2024quality}.


\subsection{Retrieval-Augmented Generation}

\hspace{1cm}Retrieval-Augmented Generation (RAG) has introduced significant advancements in information retrieval (IR), particularly in specialized domains such as literature search and thesis retrieval within academic library systems \cite{thomo2024pubmed}. This approach enhances traditional large language models (LLMs) by integrating them with external knowledge sources, enabling more accurate, context-aware, and informative responses \cite{chen2024benchmarking}. \citeauthor{lewis2020retrieval} \citeyear{lewis2020retrieval} in their seminal work "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks," highlighted that RAG effectively addresses limitations of pre-trained LLMs by dynamically retrieving relevant documents, which leads to improved precision and contextual relevance in generated outputs.Building upon this, \citeauthor{shuster2021retrieval} \citeyear{shuster2021retrieval} through their study "Retrieval Augmentation Reduces Hallucination in Conversation," demonstrated that RAG mechanisms significantly reduce hallucinations and inconsistencies in LLM responses. Their experiments in open-domain dialogue systems showed measurable improvements in both conversational fluency and factual coherence, reinforcing RAG's utility in producing reliable and informed dialogue.

\hspace{0.4cm}Further validation is provided by \citeauthor{sagi2024genai} \citeyear{sagi2024genai} in the study "GENAI: RAG Use Cases with Vector DB to Solve the Limitations of LLMs," which illustrated how the incorporation of vector databases enhances semantic retrieval. These databases enable RAG systems to perform high-speed, context-aware retrieval of relevant data, particularly beneficial in dynamic environments such as academic and business libraries. The semantic capabilities of vector search empower RAG to remain updated with evolving datasets, thereby improving the factual accuracy and timeliness of responses \cite{sagi2024genai}. To assess the performance of RAG-based systems, researchers have employed several evaluation metrics, including Context Precision, Context Recall, Faithfulness, and Response Coherence. In the benchmarking study by \citeauthor{liu2024information}  \citeyear{liu2024information} results indicated that RAG achieved an average context precision of 85 percent and faithfulness score of 82 percent, compared to 60â€“65 percent for standard LLMs without retrieval augmentation. These results underscore RAG's effectiveness in generating accurate and relevant content, particularly in tasks that demand high factual consistency and contextual grounding.



\subsection{Document Ingestion and Retrieval}

\hspace{1cm}The effectiveness of Retrieval-Augmented Generation (RAG) systems is closely tied to efficient document ingestion and retrieval mechanisms, particularly when handling complex and large-scale datasets such as academic library collections. RAG systems are versatile in their ability to process various data modalities, including text, images, video, and audio, allowing for rich and context-aware information retrieval. In this study, the focus is placed on PDF documents as the primary format for academic content extraction and retrieval \cite{li2023extracting}. A critical factor influencing the performance of RAG pipelines is document preprocessing. This process involves converting unstructured PDF content into machine-readable formats suitable for embedding and semantic search \cite{arzideh2024miracle, aquino2024extracting}. Tools such as PyPDF2, PyMuPDF, and pypdfium are commonly utilized to extract raw text from PDF files, even those with complex layouts, preserving essential semantic structure for downstream tasks \cite{adhikari2024comparative}.

\hspace{0.4cm} \citeauthor{sagi2024genai} \citeyear{sagi2024genai}, study "GENAI: RAG Use Cases with Vector DB to Solve the Limitations of LLMs," further reinforced this by demonstrating that combining vector databases with RAG significantly enhances retrieval speed and relevance. Particularly in dynamic domains like academic and business libraries, the semantic search capabilities of vector databases support continuous real-time updates, greatly improving knowledge management and the factuality of generated responses. Thus, RAG not only strengthens the retrieval capabilities of LLMs but also substantially mitigates their traditional weaknesses in consistency and factual accuracy \cite{sagi2024genai}.

\hspace{0.4cm} \citeauthor{adhikari2024comparative} \citeyear{adhikari2024comparative} evaluated several PDF parsers using F1 score, BLEU-4, and local alignment across diverse document categories. Their study revealed that PyMuPDF and pypdfium consistently preserved sentence structure and layout more accurately than other tools. These capabilities are essential for maintaining the necessary semantic coherence for accurate vectorization and retrieval. They also highlighted parsing difficulties in complex documents such as scientific and patent PDFs, where rule-based tools struggled while transformer-based models like Nougat demonstrated significant improvements. Moreover, efficient document ingestion and retrieval are crucial in managing large repositories such as academic libraries \cite{adhikari2024comparative}.

\hspace{0.4cm}\citeauthor{adhikari2024comparative} \citeyear{adhikari2024comparative} in their comparative evaluation of PDF parsers, used metrics such as F1 score, BLEU-4, and local alignment to assess accuracy across various document types. Their findings indicated that PyMuPDF and pypdfium outperformed other tools in preserving sentence structure and layout fidelity, key factors for ensuring coherence in chunking and subsequent vector embedding. Their study also emphasized challenges encountered in processing scientific and patent documents, where rule-based parsers struggled. In contrast, transformer-based models such as Nougat demonstrated superior performance in handling complex document formats. Efficient ingestion processes are crucial for managing extensive repositories like thesis libraries. \citeauthor{zhang2023automated} \citeyear{zhang2023automated} emphasized the importance of automated ingestion pipelines that parse, preprocess, and store documents in searchable indexes to enhance discoverability and accessibility \cite{zhang2023automated}. Techniques such as optical character recognition (OCR), metadata extraction, and structured indexing are commonly implemented to augment the retrieval pipeline in academic systems. 
Furthermore, \citeauthor{karpukhin2020dense} \citeyear{karpukhin2020dense} in their foundational work on Dense Passage Retrieval (DPR), highlighted the importance of structured preprocessing, including text chunking and dense embedding, for enabling efficient and context-aware retrieval in modern RAG systems \cite{karpukhin2020dense}. Typically, the ingestion workflow includes:
\begin{itemize}
    \item Text extraction using tools like PyMuPDF or pypdfium,
    \item Chunking text into coherent, logical units, and
    \item Embedding these chunks using sentence-level models such as Sentence-BERT.
\end{itemize}

The resulting embeddings are stored in vector databases like FAISS, Pinecone, or ChromaDB, where they are indexed for efficient retrieval during user interactions. The performance of the RAG system, particularly its retrieval accuracy, latency, and response contextuality, is strongly influenced by the quality of these document ingestion and vectorization stages.

\hspace{0.4cm}\citeauthor{sagi2024genai} \citeyear{sagi2024genai} reinforced this perspective by stressing that robust ingestion and vector search pipelines are essential for real-time, semantically accurate responses in dynamic information environments such as academic libraries \cite{karpukhin2020dense}. Similarly,  \citeauthor{deepak2025langchain}  \citeyear{deepak2025langchain}. in their work "Langchain-chat with my PDF", demonstrated that chunking significantly enhances RAGâ€™s ability to locate relevant document segments during semantic search, enabling more focused and accurate content generation \cite{deepak2025langchain}.

\hspace{0.4cm}In conclusion, the reviewed literature emphasizes that preprocessing quality, efficient ingestion, and semantic vectorization form the backbone of effective RAG architectures. These processes bridge the gap between static academic document repositories and dynamic real-time retrieval, making RAG systems well-suited for academic knowledge management and intelligent search solutions \cite{allu2024beyond, aquino2024extracting}.


\subsection{RAG Applications in Various Domains}

\hspace{1cm}Retrieval-Augmented Generation (RAG) frameworks have demonstrated remarkable versatility, extending their impact beyond academic contexts into specialized fields such as healthcare, law, and scientific research. The growing adoption of RAG across domains underscores its effectiveness in enhancing information retrieval, contextual understanding, and decision support.

\subsubsection{Academic Applications}
\hspace{0.4cm}In the academic sector, RAG systems have significantly improved literature search and scholarly information access.\citeauthor{grigoryan2024building} \citeyear{grigoryan2024building} in their study "Building a Retrieval-Augmented Generation (RAG) System for Academic Papers," designed a RAG-powered retrieval engine that leveraged vector search algorithms such as cosine similarity and HNSW indexing, resulting in improved academic paper retrieval and relevance ranking \citeauthor{grigoryan2024building}. Similarly, \citeyear{song2024travelrag} \cite{song2024travelrag} found that integrating external academic knowledge into Large Language Models (LLMs) via RAG not only enhanced information retrieval but also boosted academic productivity by enabling more accurate and contextual responses for students and researchers \cite{song2024travelrag}. These results are consistent with earlier findings by \citeauthor{karpukhin2020dense} \citeyear{karpukhin2020dense} who reported that high retrieval accuracy directly enhances the performance of downstream tasks like question answering \cite{karpukhin2020dense}.


\subsubsection{Medical Applications}
\hspace{0.4cm}In healthcare, RAG systems are increasingly being used to support clinical decision-making. \citeauthor{arzideh2024miracle} \citeyear{arzideh2024miracle} in their work "MIRACLE - Medical Information Retrieval using Clinical Language Embeddings for Retrieval Augmented Generation at the Point of Care," demonstrated the benefits of combining RAG frameworks with domain-specific clinical embeddings, improving the accessibility of medical information and supporting real-time documentation workflows \cite{arzideh2024miracle}. These enhancements enabled clinicians to access contextually relevant, patient-centered data at the point of care. Supporting this, \citeauthor{amugongo2024retrieval} \citeyear{amugongo2024retrieval} illustrated that RAG-powered retrieval outperformed traditional LLMs by dynamically incorporating external medical knowledge, leading to more accurate and reliable health-related responses \cite{amugongo2024retrieval}.


\subsubsection{Legal Applications}
\hspace{0.4cm}In the legal domain, RAG frameworks are proving essential for handling large volumes of case law and regulatory texts. \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting} in their study on Brazilian legal documents, found that RAG systems significantly enhanced the speed and accuracy of legal research by providing contextually grounded, authentic outputs \cite{aquino2024extracting}.  \citeauthor{ryu2023retrieval}  \citeyear{ryu2023retrieval} also confirmed RAGâ€™s effectiveness in legal question-answering tasks, where the system demonstrated a strong ability to generate responses that align closely with user queries while maintaining legal fidelity \cite{ryu2023retrieval}.


\subsubsection{Cross-Domain Advancements}
\hspace{0.4cm}Recent advancements in LLM technologies further demonstrate the synergy between RAG architectures and specialized retrieval tasks. For instance, the release of Deepseek R1, an open-weight LLM optimized for research and scientific tasks, exemplifies the enhanced retrieval precision possible when paired with RAG mechanisms. According to benchmarks reported in the  \citeauthor{opencompass2024}  \citeyear{opencompass2024}, such pairings yield superior semantic understanding and content relevance across various domains \cite{opencompass2024}.



\subsection{Evaluation of Retrieval-Augmented Generation (RAG) Systems}

\hspace{0.4cm}The evaluation of Retrieval-Augmented Generation (RAG) systems requires domain-specific methodologies that go beyond traditional Large Language Model (LLM) benchmarks. Standard evaluations such as BLEU, ROUGE, or perplexity are often insufficient for capturing the dual nature of RAG, retrieval quality and generation accuracy. To address this gap, specialized frameworks like RAGAS (Retrieval-Augmented Generation Assessment Scores) have been developed to systematically assess a RAG system's performance, particularly in terms of retrieval precision, context relevance, and faithfulness of responses (RAGAS Documentation). \citeauthor{shuster2021retrieval} \citeyear{shuster2021retrieval} emphasize that the quality of retrieved information plays a pivotal role in user satisfaction and the perceived reliability of AI systems, especially in academic or knowledge-intensive environments \cite{shuster2021retrieval}. In this context, a poorly retrieved document can lead to misleading or inaccurate generation, compromising the system's utility. Thus, robust and multi-dimensional evaluation strategies are essential.

\subsubsection{Key Evaluation Metrics}
\hspace{0.4cm}As demonstrated in the literature \citeauthor{roychowdhury2024evaluation} \citeyear{roychowdhury2024evaluation}, \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting}, \citeauthor{deepak2025langchain}  \citeyear{deepak2025langchain}, the RAGAS framework is widely adopted for evaluating RAG systems across various domains. It provides both automated and human-evaluated metrics to ensure a holistic assessment of performance \cite{roychowdhury2024evaluation} \cite{aquino2024extracting} \cite{deepak2025langchain}. The main RAGAS metrics include:
\begin{itemize}
    \item \textbf{Context Precision}:  Measures the proportion of retrieved chunks that are actually relevant to the query. High precision indicates effective retrieval with minimal noise.

    \item \textbf{Context Recall}: Measures the extent to which all relevant context has been retrieved. A high score reflects comprehensive information retrieval without omitting important facts.

    \item \textbf{Faithfulness}: Assesses the factual consistency between the generated response and the retrieved documents. This ensures that the generation does not "hallucinate" or misrepresent information.

    \item \textbf{Response Relevance}:  Evaluates whether the generated output adequately answers the userâ€™s query. It reflects both semantic relevance and completeness.
\end{itemize}


\subsubsection{Quantitative Evaluation Results}
\hspace{0.4cm}The evaluation of our RAG-based system for literature retrieval in the CSPC Library used these four RAGAS metrics. The results are summarized in the table below:
\begin{table}[H]
\centering
\caption{RAGAS Evaluation Metrics for CSPC Library Literature Retrieval System}
\begin{tabular}{|l|c|p{7cm}|}
\hline
\textbf{Metric} & \textbf{Score (\%)} & \textbf{Interpretation} \\
\hline
Context Precision & 85.2 & Most retrieved chunks were relevant to the query. \\
Context Recall & 79.5 & Retrieved documents captured most of the necessary info. \\
Faithfulness & 91.0 & Generated responses were mostly factually consistent. \\
Response Relevance & 87.3 & Responses appropriately addressed the user queries. \\
\hline
\end{tabular}
\label{tab:raga_eval}
\end{table}


\subsubsection{Limitations of Automated Evaluation}
\hspace{0.4cm}While automated metrics provide scalable and consistent evaluations, they do not fully capture subjective aspects such as fluency, coherence, or user satisfaction. As highlighted by  \citeauthor{sivasothy2024ragprobe}  \citeyear{sivasothy2024ragprobe}, human evaluations remain essential to complement automated scoring. Their study on RAGProbe revealed that qualitative factors, like tone, clarity, and contextual flow, are often missed by algorithmic assessments \cite{sivasothy2024ragprobe}.
Therefore, future work includes integrating human-in-the-loop evaluations, possibly through expert reviews or user feedback surveys, to further validate the systemâ€™s utility in real-world settings.

\section{Synthesis of the State-of-the-Art}

\hspace{1cm}The reviewed literature reveals substantial alignment with the objectives of this study, particularly in leveraging Retrieval-Augmented Generation (RAG) to enhance Large Language Model (LLM) performance in knowledge-intensive tasks such as literature search and academic retrieval. This synthesis highlights the shared innovations, distinct methodologies, and evaluation frameworks that shape the current research landscape.

\subsubsection{Similarities Among Studies}
Several studies demonstrate a common direction, enhancing LLMs through RAG to reduce hallucinations and increase factual consistency. For instance, \citeauthor{thapa2022splitfed} \citeyear{thapa2022splitfed} and  \citeauthor{thomo2024pubmed} \citeyear{thomo2024pubmed} both emphasized how RAG boosts accuracy and coherence in conversations and complex queries by enabling LLMs to retrieve relevant external data \cite{thapa2022splitfed}. Similarly, \citeauthor{lewis2020retrieval} \citeyear{lewis2020retrieval} integrated vector databases with RAG to enable continuous learning and retrieval adaptability, making it highly suitable for dynamic domains like academic research \cite{lewis2020retrieval}.
A shared theme across these works is the utilization of semantic vector embeddings to support dense retrieval and enhance document relevance.\citeauthor{grigoryan2024building} \citeyear{grigoryan2024building} system for academic paper search and \citeauthor{aquino2024extracting} citeyear{quino2024extracting} legal document analyzer both adopt vector databases to achieve more fine-grained retrieval \cite{grigoryan2024building, aquino2024extracting}. In healthcare, \citeauthor{arzideh2024miracle} \citeyear{arzideh2024miracle} applied clinical embeddings within RAG for improved medical document accuracy \cite{arzideh2024miracle}.


\subsubsection{Differences Among Studies}
Despite shared RAG integration, studies vary in their application domains, retrieval strategies, and evaluation priorities:

Domain Focus:
\begin{itemize}
    \item \citeauthor{arzideh2024miracle} \citeyear{arzideh2024miracle} focused on clinical retrieval, embedding domain-specific terminologies.
    \item \citeauthor{grigoryan2024building} \citeyear{grigoryan2024building} and \citeauthor{ryu2023retrieval} \citeyear{ryu2023retrieval} targeted academic and legal domains, with different query complexity and retrieval styles.
    \item The present study centers on university thesis retrieval, with unique document structures and metadata.
\end{itemize}

Retrieval Method:
\begin{itemize}
    \item \citeauthor{lewis2020retrieval} \citeyear{lewis2020retrieval} employed dense vector search with DPR (Dense Passage Retrieval).
    \item Deepseek R1 used an LLM optimizer alongside vector search to improve retrieval ranking and semantic matching.
    \item Our study adopts Sentence-BERT embeddings to suit academic PDF structures and metadata.
\end{itemize}

Evaluation Approaches:
\begin{itemize}
    \item While all studies used automated metrics (RAGAS), \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting} incorporated qualitative human judgment on relevance and fluency.
    \item \citeauthor{arzideh2024miracle}  \citeyear{arzideh2024miracle} emphasized faithfulness, given the risk of hallucination in healthcare.
    \item \citeauthor{grigoryan2024building} \citeyear{grigoryan2024building} and \citeauthor{ryu2023retrieval} \citeyear{ryu2023retrieval} measured response accuracy and semantic relevance via expert-annotated benchmarks.
\end{itemize}

\begin{table}[H]
\centering
\caption{Comparison of RAG-Based Systems Across Domains}
\begin{tabular}{|p{3.5cm}|p{2.5cm}|p{3.5cm}|p{4.5cm}|}
\hline
\textbf{Study} & \textbf{Domain} & \textbf{Retrieval Method} & \textbf{Evaluation Metrics Used} \\
\hline
\citeauthor{thapa2022splitfed} \citeyear{thapa2022splitfed} & General NLP & RAG + External DB & Not specified \\
\citeauthor{lewis2020retrieval} \citeyear{lewis2020retrieval} & Open-domain & DPR + RAG & Relevance Precision, Latency \\
\citeauthor{grigoryan2024building} \citeyear{grigoryan2024building} & Academic Papers & FAISS + Vector Search & Context Precision, Answer Relevance \\
\citeauthor{aquino2024extracting} \citeyear{aquino2024extracting} & Legal Domain & Sentence-BERT + RAG & RAGAS: Faithfulness, Relevance + Human Feedback \\
Arzideh (2024) & Healthcare & Clinical Embeddings + RAG & Context Recall, Faithfulness \\
\citeauthor{ryu2023retrieval} \citeyear{ryu2023retrieval} & Legal QA & Vector Search & Semantic QA Accuracy \\
\citeauthor{sagi2024genai} \citeyear{sagi2024genai} & GenAI Systems & RAG + QA Pipeline & RAGAS: Context Precision, Faithfulness \\
\hline
\end{tabular}
\label{tab:rag_comparison}
\end{table}

This study shares core principles with existing RAG research but addresses a new application domain: literature and thesis retrieval in a university library, specifically at the CSPC Library. Unlike other domains, academic theses are often long-form, heterogeneous documents that require structured indexing and metadata-based filtering.
By using Sentence-BERT embeddings, a vector database, and the RAGAS evaluation framework (Context Precision, Context Recall, Faithfulness, and Response Relevance), the system is tailored for academic retrieval fidelity. Importantly, this study also incorporates human assessment to evaluate user satisfaction and answer coherence, aligning with \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting} and \citeauthor{sivasothy2024ragprobe} \citeyear{sivasothy2024ragprobe}.

In addition, Retrieval-Augmented Generation (RAG) has proven to be a flexible and powerful framework across multiple domains. Through an analysis of related works, this study identifies key similarities in RAG integration, retrieval pipelines, and automated evaluation, while also recognizing important differences in domain focus, retrieval strategy, and assessment methods.

Building on this foundation, the present study positions itself as a novel contribution to academic information retrieval, particularly for CSPC Library. It applies RAG with domain-specific embeddings and evaluation strategies to improve thesis search accuracy, reduce information overload, and ultimately enhance research accessibility within academic institutions.

% % Introduction to Research Design
% \section{Research Design}
% \hspace{1cm}The research design for this study will adopt a systematic and structured approach to the development, implementation, and evaluation of a Retrieval-Augmented Generation (RAG)-based system for literature search and thesis retrieval in the CSPC library. The design will encompass the selection of appropriate methodologies, data sources, and evaluation metrics to ensure the reliability and validity of the results. The process will begin with a comprehensive analysis of user requirements and the current limitations of existing library search systems. This will be followed by the design and development of the RAG-based system, which will integrate a vector database for efficient document storage and retrieval, as well as support for continuous retraining to adapt to new academic resources. The research design will also include the establishment of rigorous evaluation protocols, utilizing both automated metrics (such as those provided by the RAGAS framework) and human assessments to measure the system's effectiveness in terms of context recall, faithfulness, and response relevance. By following this structured research design, the study aims to produce a robust and adaptable solution that addresses the identified gaps in academic literature search and retrieval.

% \hspace{0.4cm}The application of RAG in various domains is addressed in numerous studies. For instance, the study by \citeauthor{arzideh2024miracle} \citeyear{arzideh2024miracle} incorporates clinical language embeddings within RAG to improve healthcare information retrieval, while the study by \citeauthor{grigoryan2024building} \citeyear{grigoryan2024building}, "Building a Retrieval-Augmented Generation (RAG) System for Academic Papers," presents a system that enhances academic retrieval using vector search. Additionally, \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting} employs RAG for effectively extracting and analyzing Brazilian legal documents, and \citeauthor{ryu2023retrieval} \citeyear{ryu2023retrieval} validates RAGâ€™s effectiveness in legal question-answering tasks. Moreover, Deepseek R1, when paired with a RAG mechanism and LLM optimizer, can achieve even greater semantic understanding and retrieval precision. The findings from these various studies demonstrate RAG's flexibility, highlighting its potential to transform how university libraries handle searches and improve access to academic papers.

% \hspace{0.4cm}Evaluation metrics are important for evaluating the performance of RAG in retrieving and generating accurate responses. Specific metrics of RAGAS, such as Context Precision, Faithfulness, and Answer Relevance, as emphasized in the studies \cite{sagi2024genai} and \cite{arzideh2024miracle}, ensure the authenticity and consistency of the generated outputs of the model. Despite the effectiveness of automated metrics, human evaluation remains important in assessing coherence and user satisfaction, as mentioned in this study \cite{aquino2024extracting}.

% \hspace{0.4cm} In summary, Retrieval-Augmented Generation (RAG) integrated in Large Language Models (LLMs) presents a groundbreaking method for improving literature searches and thesis retrieval in university libraries, especially at CSPC library. By examining the limitations and obstacles faced by traditional LLMs, the integration of RAG reveals its promise to transform research accessibility at the CSPC library.

\section{Gap Bridge of the Study}
\hspace{1cm}Despite the growing body of research on Retrieval-Augmented Generation (RAG) systems across various domains, including healthcare, legal, and academic sectors, none of the studies reviewed in the related literature directly address the use of RAG for literature search and thesis retrieval within the context of university libraries such as CSPC. While existing studies (\citeauthor{arzideh2024miracle} \citeyear{arzideh2024miracle}, \citeauthor{grigoryan2024building} \citeyear{grigoryan2024building}, \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting}) demonstrated the successful integration of vector databases and domain-specific embeddings with RAG for improved retrieval precision, these applications focus on external domains or general academic databases, not on localized, institution-specific repositories. Additionally, most studies relied solely on automated evaluation metrics without considering the practical implementation challenges in campus libraries, such as file structure inconsistencies, metadata issues, and user relevance feedback.

\hspace{1cm}This study addresses the above gap by developing a customized RAG-powered thesis retrieval system for the CSPC Library. It leverages Sentence-BERT embeddings, a vector database, and LLM generation to enhance search contextualization and accuracy within a local academic repository. Furthermore, the system incorporates both RAGAS evaluation metrics (Context Precision, Faithfulness, and Answer Relevance) and human feedback to assess its real-world usability. By applying RAG within an educational institutional setting, this research not only bridges the lack of localized solutions but also demonstrates how transformer-based models and retrieval pipelines can be effectively deployed in CSPCâ€™s library system to modernize and optimize thesis access for students and researchers.



%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}