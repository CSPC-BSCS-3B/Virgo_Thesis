\chapter{Related Literature and Studies}
\begin{refsection}

This chapter reviews related literature and existing systems on the study. It provides a synthesis of related works, an overview of state-of-the-art technologies and methodologies, and underline the research gaps that the present study has addressed.

\section{Review of Related Literature and Studies}
A review of books, scholarly articles, journals, and previous thesis projects concerning the research topic was carried out to develop an in depth understanding of the subject of the study. The findings are organized thematically in line with the key areas of the study.

\subsection{Large Language Models}

Large Language Models (LLMs) have significantly improved the use case of information retrieval (IR) within academic settings. The integration of LLMs, like ChatGPT and other model architectures, offers notable advancements in natural language processing (NLP) and also proves its capabilities to enhance IR, question-answering, summarization, and content generation, which benefits academic environments where efficient access to information is crucial \cite{yalamanchili2024quality} \cite{yang2023large}. Recent works by \citeauthor{khraisha2024can} \citeyear{khraisha2024can} and \citeauthor{gartlehner2023data} \citeyear{gartlehner2023data} have demonstrated how large language models can automate research tasks such as systematic review, data extraction, and document screening. This suggests that LLMs are capable of improving research productivity in academia \cite{khraisha2024can}  \cite{gartlehner2023data}.

Even though Large language models offer a number of advantages with regard to information retrieval, they also present significant challenges. Among the major challenges is that they are inefficient at executing domain specific tasks for which specialized knowledge is required. This is because these models source knowledge from pre-training, hence LLMs  are limited to offer fact based responses within certain domains such as academia. Omar et al. mentioned in their study that LLMs, including ChatGPT, could be helpful in specialized domains to serve complementary purposes but may fail in complicated queries because they have not seen enough training data related to those fields \cite{khraisha2024can}. Moreover, pre-trained LLMs have difficulty keeping on pace with continuous data growth in various domains, thus, it is impossible for them to refresh their knowledge without extensive fine-tuning. Lucas et al. point out that this inability-in an academic and professional sense-of large language models to access up-to-date domain-specific repositories significantly diminishes their effectiveness and value \cite{gartlehner2023data}.

Despite of LLMs being at the lead of NLP innovation, their actual application in domain-specific tasks is deeply prevented by certain challenges. These include real-time data availability, dependence on pre-trained knowledge bases, and ethical concerns pertaining to data privacy. Innovation around these challenges using novel methodologies, such as the retrieval-augmented generation approach, increases the capability of models to meet specialized applications that have strict requirements.

\subsection{Retrieval-Augmented Generation}

RAG has achieved remarkable improvements in the IR domain, especially in tasks regarding literature search and thesis retrieval in library systems \cite{thomo2024pubmed}. The architecture supports the traditional large language models with external knowledge sources for enhancing the relevance, richness, and correctness of the responses \cite{chen2024benchmarking}.

As articulated in the studies of \citeauthor{lewis2020retrieval} \citeyear{lewis2020retrieval} titled "Retrieval-Augmented Generation for Knowledge Intensive NLP Tasks," RAG provides for much more accurate responses because it helps in addressing certain inherent limitations of LLMs, especially in the areas of accurate knowledge retrieval and context relevance. Another articulation about the topic is the the study of \citeauthor{shuster2021retrieval} \citeyear{shuster2021retrieval}, titled "Retrieval Augmentation Reduces Hallucination in Conversation," go on to show that RAG reduces inconsistencies and hallucinations in the LLM outputs. Their results show that RAG mechanisms increase conversational fluency and integrity, particularly in open domain conversational settings, bringing about knowledgeable and more coherent responses.

Moreover, it has recently been further supported by work titled "GENAI: RAG Use Cases with Vector DB to Solve the Limitations of LLMs," in which the authors demonstrate that the integration of vector databases with RAG essentially improves retrieval speed and relevance.Semantic search using vector databases makes continuous real-time updates possible for dynamic domains such as business and academic libraries, resulting in a high level of knowledge management and factual correctness in the generated responses. Thus, RAG strengthens not only the capability of LLM retrieval but also addresses the essential weaknesses of LLMs: consistency and factuality \cite{sagi2024genai}.

\subsection{Document Ingestion and Retrieval}

Successful execution of RAG systems relies on utilizing documents in an effective manner and carrying out robust retrieval procedures, particularly in addressing large and complex datasets found in academic libraries. RAG systems can use any data source, including text, video, images, and audio, thus allowing flexible and contextually rich information retrieval. In study this about RAG, the main corpus mostly used is PDF documents to extract academic content which focuses by this authors \cite{li2023extracting}.

The RAG chatbot rely its effectiveness by depending on the quality of preprocessing, which involves converting unstructured PDF data into machine readable formats suitable for embedding and semantic search \cite{arzideh2024miracle} \cite{aquino2024extracting}. The PyPDF2, PyMuPDF, and pypdfium are the commonly used tools for this task in most studies to help in extracting raw text from complex PDF layouts \cite{adhikari2024comparative}.

A study by \citeauthor{sagi2024genai} \citeyear{sagi2024genai}, "GENAI: RAG Use Cases with Vector DB to Solve the Limitations of LLMs," further corroborates this idea by showing how the introduction of vector databases significantly improves retrieval speed and relevance when integrated into RAG. The semantic search capabilities of the vector databases in turn support continuous real-time updates in dynamic domains like academic and business libraries, hence greatly improving knowledge management and factual accuracy of responses generated. Therefore, RAG enhances not only the retrieval capabilities of LLMs but also considerably mitigates its traditional deficiencies in consistency and factual accuracy \cite{sagi2024genai}.

\bigbreak
\citeauthor{adhikari2024comparative} (\citeyear{adhikari2024comparative}) into their examined different kind of PDF parsers using F1 score, BLEU-4, and local alignment on a wide range of document types. The results show that PyMuPDF and pypdfium more reliable to retain sentence structure and format than the other tools, something highly recommended for retaining semantic coherence and offering proper vectorization and retrieval. One can also observe parsing difficulties posed by complex documents, such as scientific articles and patent PDFs, for which rule-based tools significantly underperform versus transformer-based models. Furthermore, the efficiency of document intake and retrieval becomes crucial for large repositories, including academic libraries \cite{adhikari2024comparative}.

As pointed out by \citeauthor{zhang2023automated} \citep{zhang2023automated}, automated ingestion pipelines parsing documents into a searchable database improve the discoverability and accessibility of scholarly content.

Techniques like OCR, metadata extraction, and structured indexing are common practices employed on thesis repositories to enable various retrieval operations easily \cite{zhang2023automated}. Along related lines, \citeauthor{karpukhin2020dense} (\citeyear{karpukhin2020dense}) stress the importance of document preprocessing, chunking, and embedding to permit semantic search in DPR and, consequently, for modern RAG \cite{karpukhin2020dense}. Generally, this ingestion process consists of several steps: (1) extracting text with PyMuPDF or pypdfium, among other tools; (2) chunking the text into smaller, logically coherent pieces; and (3) embedding by models such as Sentence-BERT. Later, these vectors will be kept in specialized vector databases (e.g., FAISS, Pinecone) so that the actual retrieve action when users make inquiries will be fast and efficient. Hence, robust document ingestion and storage directly impact retrieval accuracy, system responsiveness, and user experience. Drawing on Sagi's study, it is clear that through the effectiveness of ingestion and vectorization, retrieval of relevant information is very fast and returns very accurate context enriched responses from RAG models, especially in dynamic settings such as academic libraries \cite{karpukhin2020dense}.

\citeauthor{deepak2025langchain} (\citeyear{deepak2025langchain}) explained, in "LangChain-Chat with My PDF", that the process mainly used in the handling of PDFs involves embedding and chunking, all under vectorization techniques. Their study reveals how chunking enables RAG to find relevant parts of the document for specific user queries in order to enhance handling substantial PDF based information and enrich the system's semantic search capabilities \cite{deepak2025langchain}. 

\bigbreak
Collectively, this body of work suggests that strong preprocessing, ingestion, and vectorization form is crucial for bridging static document repositories with real-time information retrieval, demonstrating the potential for RAG architectures in managing large collections of academic knowledge \cite{allu2024beyond} \cite{aquino2024extracting}.

\subsection{RAG Applications in Various Domains}

Beyond contexts in the field of academics, RAG frameworks are increasingly applied to specialized domains, including legal research, medical information retrieval, and scientific literature search, underlining broad versatility and impact.

In the academic domain, \citeauthor{grigoryan2024building} \citeyear{grigoryan2024building}, in their work "Building a Retrieval-Augmented Generation (RAG) System for Academic Papers," proposed a RAG-enabled system that significantly enhanced the retrieval of scholarly literature by using vector search techniques like cosine similarity and HNSW indexing. \citeauthor{song2024travelrag} \citeyear{song2024travelrag} support the fact that RAG architectures allow for improving not only the process of search but also academic output in general, since large language models receive external knowledge and can provide more accurate and efficient information retrieval for students and researchers.This conclusion is supported by \citeauthor{karpukhin2020dense} \citeyear{karpukhin2020dense}, who noticed that higher information retrieval accuracy corresponds to better search results and improved model performance in question answering.

\citeauthor{arzideh2024miracle} \citeyear{arzideh2024miracle} underline that in healthcare contexts, RAG is very effective within the scope of domain specific clinical embeddings. Their work, "MIRACLE-Medical Information Retrieval using Clinical Language Embeddings for Retrieval Augmented Generation at the Point of Care," significantly enhances clinical decision making, improves workflows in clinical documentation, and personalizes access to health information. On the basis of the above statement, \citeauthor{amugongo2024retrieval} \citeyear{amugongo2024retrieval} note that the RAG architecture is able to retrieve external medical data in a very effective manner, thus providing responses with high levels of accuracy and reliability while minimizing the limitations characteristic of traditional large language models. Similarly, in the legal domain, a study by \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting} notes that RAG systems greatly enhance legal research by speeding up the retrieval of cases and statutes and enhancing the authenticity and contextual accuracy of the output.

Recent works, including Google Gemini, which is a more advanced large language model, prove that using this model with a retrieval augmented generation framework allows for better semantic understanding and improving retrieval accuracy \citeauthor{prabhulal2025ragpipeline} \citeyear{prabhulal2025ragpipeline}. This is further supported by previous work into RAG pipelines \cite{prabhulal2025ragpipeline}. Parallel to this, vector search provides a conceptual framework for building intelligent, document aware systems where the integration of high quality semantic embeddings with indexing helps ensure that the responses are accurate, transparent, and drawn from domain specific data rather than from general model knowledge.

\subsection{Evaluation of Retrieval-Augmented Generation (RAG) Systems}

Evaluation of RAG systems requires methodological extensions beyond those applied in traditional designs for large language models. The RAGAS framework presents a structured way of evaluating the context precision, contextual relevance, and faithfulness in a generated response. Studies of \citeauthor{shuster2021retrieval} \citeyear{shuster2021retrieval} show that high quality retrieval has a great effect on user satisfaction and perceived reliability with respect to conversational AI. This, therefore, underlines the need for specialized evaluation frameworks which can guarantee the effectiveness of RAG systems.

In this specialized needs, metrics tailored for RAG models come into pivotal consideration. One of the widely used approaches is the RAGAS evaluation framework that provides key metrics such as Context Recall, Faithfulness, and Response Relevance. These metrics assess the degree to which the retrieved documents substantiate the generated response \cite{roychowdhury2024evaluation}.Context Precision characterizes the share of relevant chunks inside the contexts retrieved, while Context Recall ensures that no important information is missed. Faithfulness checks factual coherence between generated responses and the respective retrieved documents, while Response Relevance checks if the response correctly addresses user query relevance \cite{aquino2024extracting} \cite{deepak2025langchain}.

However, while automated measures may be more valid for some purposes, they often lack coverage of the qualitative dimensions of consistency, fluency, and overall user satisfaction.

Human evaluation, according to \citeauthor{sivasothy2024ragprobe} \citeyear{sivasothy2024ragprobe}, is necessary in view of improving these systems, since it considers factors that the automated approaches may fail to address.


\section{Synthesis of the State-of-the-Art}

The related literature and systems discussed have substantial relevance to the problem of the study. To have a clear understanding of this literature and studies, the researchers made a synthesis in the succeeding discussions.

Large Language Models (LLMs) with integrated RAG techniques have greatly improved the knowledge-intensive NLP tasks, overcoming LLMs' challenges. Studies \cite{thapa2022splitfed} and \cite{thomo2024pubmed} underline how combining RAG with LLMs significantly improves accuracy and coherence in conversations and complex queries. The advantage of this technique enables LLMs to retrieve relevant external data, reducing hallucinations and improving factual consistency. Furthermore, \cite{lewis2020retrieval} discussed the application of vector databases for continuous integration with RAG, which shows a significant improvement in both retrieval efficiency and the relevance of output produced by large language models. This topic plays a major role in literature searching and retrieving theses from university libraries.

A collection of various studies compiled on the use of RAG in different domains is discussed below. For instance, \citeauthor{arzideh2024miracle} \citeyear{arzideh2024miracle} incorporated clinical language embeddings into RAG for better healthcare information retrieval. \citeauthor{grigoryan2024building} \citeyear{grigoryan2024building}, in "Building a Retrieval-Augmented Generation (RAG) System for Academic Papers," proposes a system that augments academic retrieval through vector search. Further, \citeauthor{aquino2024extracting} \citeyear{aquino2024extracting} used RAG on the effective extraction and analysis of Brazilian legal documents. On the other hand, \citeauthor{ryu2023retrieval} \citeyear{ryu2023retrieval} evaluated RAG for its effectiveness in performing question and answering tasks related to the law. Finally, Google Gemini can achieve high semantic understanding, precise retrieval, and accurate, explainable, and grounded in domain data responses when combined with a RAG framework and complemented by vector search.

Together, these reports illustrate the adaptability of RAG and its potential to transform how university libraries search for and provide access to academic theses.

Evaluation metrics are important for evaluating the performance of RAG in retrieving and generating accurate responses. Specific metrics of RAGAS, such as Context Precision, Faithfulness, and Answer Relevance, as emphasized in the studies \cite{sagi2024genai} and \cite{arzideh2024miracle}, ensure the authenticity and consistency of the generated outputs of the model. Despite the effectiveness of automated metrics, human evaluation remains important in assessing coherence and user satisfaction, as mentioned in this study \cite{aquino2024extracting}.

In summary, Retrieval-Augmented Generation (RAG) integrated in Large Language Models (LLMs) presents a groundbreaking method for improving literature searches and thesis retrieval in university libraries, especially at CSPC library. By examining the limitations and obstacles faced by traditional LLMs, the integration of RAG reveals its promise to transform research accessibility at the CSPC library.

\section{Gap Bridge of the Study}

% in first gap, the gap of the papers you put in the RRL chapter. Second gap, your reason of ur gap

Existing literatures has explored the applicability of RAG systems in domains as varied as health, case law research, and academic literature retrieval, it is concerning to note that there's a gap in the way of specific applications of these systems to academic libraries for enhancing literature searches and thesis retrievals. While previous studies have shown how well RAG improves information retrieval, not much has been done in implementing this within university libraries, where there is a unique challenges and requirements that such implementations needed.

This study tries to fill this gap by designing a RAG-based chatbot system specific to the CSPC library. By focusing on the unique challenges and demands of academic libraries, this paper seeks to add substantial value in understanding the appropriate application of RAG systems toward improvement of information retrieval.


%=======================================================%
%%%%% Do not delete this part %%%%%%
\clearpage

\printbibliography[heading=subbibintoc, title={\centering Notes}]
\end{refsection}